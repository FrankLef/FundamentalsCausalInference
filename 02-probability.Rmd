# Conditional Probability and Expectation {#probability}

```{r include=FALSE}
library(tidyr)
library(dplyr)
library(fciR)
options(dplyr.summarise.inform = FALSE)
```


```{r include=FALSE}
# directory of data files
dir_data <- file.path(getwd(), "data")
# directory for functions
dir_lib <- file.path(getwd(), "lib")
```


The mortality data was given in chapter 1, section 1.2.1.

```{r}
data("mortality", package = "fciR")
dataMortality <- mortality
```


## Conditional Probability


### Law of total probability

It is important to note that $\sum_i{H_i} = H$, that is $H$ can be partitioned 
in $i$ non-overlapping partitions.

Then the law of total probabilities is

$$
\begin{align*}
P(A) &= \sum_i{P(A \cap H_i)}= \sum_i{P(A \mid H_i) P(H_i)} \\
&\text{and we condition the whole expression with B} \\
P(A \mid B) &= \sum_i{P(A \cap H_i \mid B)}= \sum_i{P(A \mid B, H_i) P(B,H_i)} \\
\end{align*}
$$

and the multiplication rule is


$$
\begin{align*}
P(A, B \mid C) &= \frac{P(A, B, C)}{P(C)} \\
&= \frac{P(A \mid B, C) P(B, C)}{P(C)} \\
&= \frac{P(A \mid B, C) P(B \mid C) P(C)}{P(C)} \\
&= P(A \mid B, C) P(B \mid C)
\end{align*}
$$



## Conditional Expectation and the Law of Total expectation

The conditional expectation is defined as

$$
E(Y \mid T) = \sum_y y P(Y=y \mid T)
$$

and one way that helps me usually understand it well is that conditioning is
the same as *filtering* the data.

For example the conditional expectation of mortality of US resident at the 
beginning of 2019 $E(Y \mid T = 1) = 0.0088$

```{r}
data("mortality_long", package = "fciR")
mortality_long |>
  filter(`T` == 1) |>
  mutate(prob = n / sum(n)) |>
  group_by(Y) |>
  summarize(EYT1 = sum(Y * prob)) |>
  pull() |>
  setNames(nm = c("EY0T1", "EY1T1"))
```

where $E(Y=0 \mid T=1) = 0$ because when $Y=0 \implies 0 \cdot P(Y=0) = 0$ and
*since $Y$ is binary* $E(Y=1 \mid T=1) = P(Y=1 \mid T=1)$.

> Analogous to the law of total probability is the law of ttal expectation,
also called double expectation theorem.

This law is *used extensively in this textbook*.


$$
\begin{align*}
E(Y \mid T) &= E_{H \mid T}(E(Y \mid H, T)) \\
&= \sum_h \left[ \sum_y y P(Y=y \mid H=h, T) \right] P(H=h \mid T)
\end{align*}
$$
Also the following equivalence is used very often in this book.

$$
\begin{align*}
E(H \mid T) = \sum_h h P(H=h \mid T) = E_{H \mid T} (H)
\end{align*}
$$

### Mean independence and conditional mean independence

> The random variable $Y$ is mean independent of $T$ if

$$
E(Y \mid T) = E(Y)
$$

> and is conditionally mean independent of $T$ given $H$ is

$$
E(Y \mid T, H) = E(Y \mid H)
$$

and the *conditional uncorrelation* and uncorrelation are

$$
E(YT \mid H) = E(Y \mid H)E(T \mid H) \implies \text{conditionally uncorrelated} \\
E(YT) = E(Y)E(T) \implies \text{uncorrelated}
$$

### Regression model

> A statistical model for a conditional expectation is called a 
*regression model*.


### Nonlinear parametric models

The function `expit()` used by the author is actually the same as
`gtools::inv.logit()`, `boot::inv.logit()` or `stats::plogis()`. In this project 
we use `stats::plogis()` to minimize dependencies since it is in base R.



## Estimation

Using the *What-if* example we have

$$
\begin{align*}
X_i \beta &= X_{i,1} \beta_i +\ldots+X_{i, p}\beta_p \\
&\therefore \\
X_i \beta &= \beta_1 A_i + \beta_2 T_i + \beta_3 H_i \\
\end{align*}
$$

and 

$$
X_i^T(Y_i - X_i \beta) \\
\therefore \\
\begin{bmatrix}
1 \cdot(Y_i - \beta_1 - A_i \beta_2 - T_i \beta_3 - H_i \beta_4) \\
A_i \cdot(Y_i - \beta_1 - A_i \beta_2 - T_i \beta_3 - H_i \beta_4) \\
T_i \cdot(Y_i - \beta_1 - A_i \beta_2 - T_i \beta_3 - H_i \beta_4)\\
H_i \cdot(Y_i - \beta_1 - A_i \beta_2 - T_i \beta_3 - H_i \beta_4)
\end{bmatrix}
$$



## Sampling Distributions and the Bootstrap

The `sim()` function in section 2.4, p. 31 is coded in `fciR::sim_intervals()`
and its alias `fciR::sim()`.


Run and verify with author's results on p. 32


```{r}
d <- sim(nsim = 500, n = 500)
stopifnot(abs(d$bad - 0.8374) < 0.03, 
          abs(d$good - 0.948) < 0.03)
d
```

and `lmodboot()` is `fciR::boot_lmod()` with the alias `fciR::lmodboot()`


Run and verify against the author's results on p.34

```{r}
data("whatifdat", package = "fciR")
message("this takes 10 sec.: we load a saved file instead")
# startTime <- Sys.time()
# out <- fciR::boot_est(whatifdat, func = fciR::prob_lmod, R = 500, conf = 0.95,
#                 inv = "expit", evars = "logit",
#                 outcome.name = "Y",
#                 input.names = c("T", "A", "H"))
# endTime <- Sys.time()
# print(endTime - startTime)
a_file <- file.path(dir_data, "chap02_lmodboot.rds")
# saveRDS(out, file = a_file)
out <- readRDS(file = a_file)

# check results with book
stopifnot(abs(out["est"] - 0.60596) < 0.02,
          abs(out["lci"] - 0.41638) < 0.02,
          abs(out["uci"] - 0.76823) < 0.02)
```


## Exercises

The exercises are located in a separate project.
