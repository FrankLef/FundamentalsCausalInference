# Backdoor Method via Standardization {#backdoor}


```{r include=FALSE}
library(tidyr)
library(dplyr)
library(ggplot2)
library(fciR)
options(dplyr.summarise.inform = FALSE)
```



```{r include=FALSE}
# directory of data files
dir_data <- file.path(getwd(), "data")
# directory for functions
dir_lib <- file.path(getwd(), "lib")
```

:::: {.center data-latex=""}

::: {.minipage data-latex="{.5\linewidth}"}

Important note on the notation used. When the author uses $E(Y=t) \mid T=t, H=h)$
it means that we condition the data on $H=h$ and we intervene on the $T$ column 
and set it to $T=t$.

For example for equation (6.2) we have

$$
\begin{align*}
E(Y(t)) &= E_H(E(Y(t) \mid H)) \\
&= E_H(E(Y(t) \mid T = t, H))
\end{align*}
$$

which indicates that $T=t$ means that we set $T=t$, i.e. *it is not a condition*
that doesn't involve a filter on the data. We know that because we have the
$E(Y(t))$ which tells us that.

But then we continue with proof (6.2) by adding the thrd line

$$
\begin{align*}
E(Y(t)) &= E_H(E(Y(t) \mid H)) \\
&= E_H(E(Y(t) \mid T = t, H)) \\
&= E_H(E(Y \mid T = t, H))
\end{align*}
$$

and, for the unwary beginner,  $E(Y \mid T = t, H)$ could mean that we are
*conditioning* on $T=t$, that is, we filter the $T$ variable in the data. This
is confusing.

To facilitate the reading and learning experience in this study project, whenever
such confusion happens, the notation fro Pearl, using the $do()$ operator 
will be used.

For example, the proof (6.2) becomes


$$
\begin{align*}
E(Y(t)) &= E_H(E(Y(t) \mid H)) \\
&= E_H(E(Y(t) \mid T = t, H)) \\
&\text{and we use the do() operator to make it clear} \\
&\text{that T=t is not a condition, it is an intervention} \\
&\text{whereas H is a condition} \\
&= E_H(E(Y \mid do(T = t), H))
\end{align*}
$$
:::

::::


## Standardization via Outome Modeling

> Standardization vis via outcome modelingis one way to estimate $E(Y(t))$


$$
\begin{align*}
&\text{by double expectation theorem} \\
&E(Y(t)) = E_H E(Y(t) \mid H) \\
&\text{by independence of T given H, (6.1)} \\
&= E_HE(Y(t) \mid T=t, H) \\
&\text{by consistency assumption} \\
&= E_HE(Y \mid do(T=t), H)
\end{align*}
$$

and with a binary data set we can write

$$
\begin{align*}
E_H E(Y \mid do(T=t), H) = E(Y \mid do(T=t), H = 0) P(H = 0) + E(Y \mid do(T=t), H = 1) P(H = 1)
\end{align*}
$$


and using the example on p. 100 with the mortality data we first load the
data set

```{r}
data("mortality_long", package = "fciR")
mortality <- mortality_long
```

and we begin by calculating $\hat{E}(Y \mid T=0, H=0)$

```{r}
mortality %>%
  filter(`T` == 0, H == 0) %>%
  summarize(EY = weighted.mean(Y, n))
```

and for all permutations of $T$ and $H$ we have


```{r}
EYcondTH <- mortality %>%
  group_by(`T`, H) %>%
  summarize(EYcond = weighted.mean(Y, n))
EYcondTH
```

and then we multiply the conditional expectations by the probabilities of $H$.

```{r}
PH <- mortality %>%
  group_by(H) %>%
  summarize(prob = sum(p))
PH
```

and the multiplication

```{r}
EYH <- dplyr::inner_join(EYcondTH, PH, by = c("H")) %>%
  mutate(EYH = EYcond * prob)
EYH
```

and the final results are

```{r}
EYout <- EYH %>%
  group_by(`T`) %>%
  summarize(EYout = sum(EYH))
EYout
```

Now, lets do it with raw data. For that we convert the mortality data to have
1 line per 10000 observations.


```{r}
mort <- mortality %>%
  select(H, `T`, Y, n) %>%
  mutate(n = as.integer(n / 10000)) %>%
  uncount(n)
str(mort)
```

and the function used to automate the process described above is as follows

```{r}
est_out_np <- function(data, outcome, exposure, confound) {
  # compute the frequencies
  summ <- data %>%
    count({{outcome}}, {{exposure}}, {{confound}}) %>%
    mutate(freq = n / sum(n))
  
  # the expected value of the outcome given the exposure and confounds
  EYcond <- summ %>%
    group_by({{exposure}}, {{confound}}) %>%
    summarize(EYcond = weighted.mean(x = {{outcome}}, w = n))
  
  # the probabilities of the confound
  PH <- summ %>%
    group_by({{confound}}) %>%
    summarize(prob = sum(freq))
  
  # multiply the conditional expectation by the confound probabilities
  cnf <- enquo(confound)
  EY <- dplyr::inner_join(EYcond, PH, by = quo_name(cnf)) %>%
    mutate(EY = EYcond * prob) %>%
    group_by({{exposure}}) %>%
    summarize(EY = sum(EY)) %>%
    arrange({{exposure}}) %>%
    pull(EY) %>%
    setNames(c("EY0", "EY1"))
}
```



```{r}
mort.out.est <- est_out_np(mort, outcome = Y, exposure = `T`, confound = H)
mort.out.est
```

and we can see it gives the same results (the function is also found in 
`fciR::backdr_out_np`)

```{r}
message("This takes about 22 min. Load from file.")
# startTime <- Sys.time()
# mort.out.np <- fciR::backdr_out_np(mort, outcome = Y, exposure = `T`, confound = H,
#                                      R = 500)
# endTime <- Sys.time()
# print(endTime - startTime)
a_file <- file.path(dir_data, "chap06_mort_out_np.rds")
# saveRDS(mort.out.np, file = a_file)
mort.out.np <- readRDS(file = a_file)
fciR::gt_measures(mort.out.np,  digits = 6,
            title = "Mortality", 
            subtitle = paste("Mortality", 
                              "Standardized Estimates",
                              "Non Parametric Without Regression",
                             sep = "<br>"))
```

and we now do it with the function `fciR::backdr_out_npr`. That function
works exactly as the function `standr` in the book.

```{r}
message("This takes about 30 min. Load from file.")
# startTime <- Sys.time()
# mort.out.npr <- fciR::backdr_out_npr(mort, formula = Y ~ `T` + H + `T`:H,
#                                      R = 500)
# endTime <- Sys.time()
# print(endTime - startTime)
a_file <- file.path(dir_data, "chap06_mort_out_npr.rds")
# saveRDS(mort.out.npr, file = a_file)
mort.out.npr <- readRDS(file = a_file)
fciR::gt_measures(mort.out.npr,  digits = 6,
            title = "Mortality", 
            subtitle = paste("Mortality", 
                              "Standardized Estimates",
                              "Non Parametric With Regression",
                              sep = "<br>"))
```

and the results are the same again.  In conclusion pretty much any of the
function.  The function `fciR::backdr_out_npr` seems faster.  The function
`fciR::backdr_out_np` is actually useful as a double check and it actually
uses a "pure" application of probabilities.

### Examples {-}

#### What-if? Study {-}

```{r}
data("whatifdat", package = "fciR")
```


##### Non-parametric With Regression `backdr_out_npr` {-}

```{r}
whatif.out <- fciR::backdr_out_npr(whatifdat, formula = Y ~ A + H + A:H)
# whatif.out
```

and we compare with the author's

```{r}
comp <- data.frame(
  name = c("EY0", "EY1", "RD", "RR"),
  auth = c(0.375, 0.289, -0.086, 0.77),
  est = whatif.out$est[whatif.out$name %in% c("EY0", "EY1", "RD", "RR")]
)
stopifnot(sum(abs(comp$auth - comp$est)) < 0.01)
```

and the results are presented in table 6.1

```{r}
fciR::gt_measures(whatif.out, 
            title = "Table 6.1", 
            subtitle = paste("What-If Study", "Standardized Estimates",
                             sep = "<br>"))
```


where we observe a reduction of the viral load but the difference is not
statistically significant.

##### Non-parametric Without Regression `backdr_out_np` {-}


```{r}
message("This takes about 15 sec.. Load from file.")
# startTime <- Sys.time()
# whatif.out.np <- fciR::backdr_out_np(whatifdat, outcome = Y, exposure = A, confound = H,
#                                      R = 500)
# endTime <- Sys.time()
# print(endTime - startTime)
a_file <- file.path(dir_data, "chap06_whatif_out_np.rds")
# saveRDS(whatif.out.np, file = a_file)
whatif.out.np <- readRDS(file = a_file)
fciR::gt_measures(whatif.out.np, 
            title = "Table 6.1", 
            subtitle = paste("What-If Study", 
                              "Standardized Estimates",
                              "Non Parametric Without Regression",
                             sep = "<br>"))
```


*T0.he results are different when we don't use linear regression!*. Not
a very large difference but a significant one.


#### Double What-if? Study {-}


```{r}
data("doublewhatifdat", package = "fciR")
# dataDoubleWhatIf <- doublewhatifsim()
```

##### Non-parametric With Regression `backdr_out_npr` {-}

```{r}
doublewhatif.out <- fciR::backdr_out_npr(doublewhatifdat, formula = VL1 ~ A + AD0 + A*AD0)
```


```{r}
fciR::gt_measures(doublewhatif.out, 
            title = "Table 6.2", 
            subtitle = paste("Double What-If Study", 
                             "Standardized Estimates with <em>H = AD0</em>",
                             "Non Parametric With Regression",
                             sep = "<br>"))
```


##### Non-parametric Without Regression `backdr_out_np` {-}


```{r}
message("This takes about 15 sec.. Load from file.")
# startTime <- Sys.time()
# doublewhatif.out.np <- fciR::backdr_out_np(doublewhatifdat, 
#                                      outcome = VL1, exposure = A, confound = AD0,
#                                      R = 500)
# endTime <- Sys.time()
# print(endTime - startTime)
a_file <- file.path(dir_data, "chap06_doublewhatif_out_np.rds")
# saveRDS(doublewhatif.out.np, file = a_file)
doublewhatif.out.np <- readRDS(file = a_file)
fciR::gt_measures(doublewhatif.out.np, 
            title = "Table 6.1", 
            subtitle = paste("Double What-If Study", 
                              "Standardized Estimates",
                              "Non Parametric Without Regression",
                             sep = "<br>"))
```

and the results are the same


> For comparisons, we repeat the standardization with $H = VL_0$



```{r}
doublewhatif.out <- fciR::backdr_out_npr(doublewhatifdat, formula = VL1 ~ A + VL0 + A*VL0)
```


```{r}
fciR::gt_measures(doublewhatif.out, 
            title = "Table 6.3", 
            subtitle = paste("Double What-If Study", 
                             "Standardized Estimates with <em>H = VL0</em>",
                             "Non Parametric With Regression",
                             sep = "<br>"))
```


### Average Effect of Treatment on the Treated


The function `bootstandatt` described in section 6.1.1 is not necessary,
see the function `bootstand` in the previous section which can do it with the 
addition of the argument `att`.


#### What-if? Study {-}


```{r}
whatif.att <- fciR::backdr_out_npr(whatifdat, formula = Y ~ A + H + A*H, 
                                   att = TRUE)
```

and we compare with the author's

```{r}
comp <- data.frame(
  name = c("EY0", "EY1", "RD", "RR"),
  auth = c(0.361, 0.276, -0.085, 0.765),
  est = whatif.att$est[whatif.att$name %in% c("EY0", "EY1", "RD", "RR")]
)
stopifnot(sum(abs(comp$auth - comp$est)) < 0.01)
```

and the results are presented in table 6.1

```{r}
fciR::gt_measures(whatif.att, 
            title = "Table 6.4", 
            subtitle = paste("What-If Study", "Standardized ATT estimates",
                             "Non Parametric With Regression",
                             sep = "<br>"))
```

It can also be done using a non-parametric method without regression. That
is following pure probabilities from the data.


```{r}
message("This takes about 8 sec.. Load from file.")
# startTime <- Sys.time()
# whatif.out.att.np <- fciR::backdr_out_np(whatifdat, outcome = Y, exposure = A, 
#                                          confound = H, att = TRUE, R = 250)
# endTime <- Sys.time()
# print(endTime - startTime)
a_file <- file.path(dir_data, "chap06_whatif_out_att_np.rds")
# saveRDS(whatif.out.att.np, file = a_file)
whatif.out.att.np <- readRDS(file = a_file)
fciR::gt_measures(whatif.out.att.np, 
            title = "Table 6.4", 
            subtitle = paste("What-If Study", 
                              "Standardized ATT Estimates",
                              "Non Parametric Without Regression",
                             sep = "<br>"))
```


#### Double What-if? Study {-}


```{r}
doublewhatif.att <- fciR::backdr_out_npr(doublewhatifdat, formula = VL1 ~ A + AD0 + A*AD0,
                              att = TRUE)
```


```{r}
fciR::gt_measures(doublewhatif.att, 
            title = "Table 6.5", 
            subtitle = paste("Double What-If Study", 
                             "Standardized ATT Estimates with <em>H = AD0</em>",
                             "Non Parametric With Regression",
                             sep = "<br>"))
```




```{r}
doublewhatif.att <- fciR::backdr_out_npr(doublewhatifdat, formula = VL1 ~ A + VL0 + A*VL0,
                              att = TRUE)
```


```{r}
fciR::gt_measures(doublewhatif.att, 
            title = "Table 6.6", 
            subtitle = paste("Double What-If Study", 
                             "Standardized ATT Estimates with <em>H = VL0</em>",
                             "Non Parametric With Regression",
                             sep = "<br>"))
```

### Standardization with a Parametric Outcome Model

For a the parametric outcome model `fciR::backdr_out()` is used


#### What-if? Study {-}

```{r}
data("whatif2dat", package = "fciR")
whatif2.out <- fciR::backdr_out(whatif2dat, formula = vl4 ~ A + lvlcont0)
# whatif2.out
```

and we compare with the author's

```{r}
comp <- data.frame(
  name = c("EY0", "EY1", "RD", "RR"),
  auth = c(0.360, 0.300, -0.061, 0.831),
  est = whatif2.out$est[whatif2.out$name %in% c("EY0", "EY1", "RD", "RR")]
)
# comp
stopifnot(sum(abs(comp$auth - comp$est)) < 0.01)
```

and the results are presented in table 6.1

```{r}
fciR::gt_measures(whatif2.out, 
            title = "Table 6.7", 
            subtitle = paste("What-If Study", 
                             "Outome-model Standardization with <em>H = lvlcont0</ems>",
                             sep = "<br>"))
```

#### General Social Survey {-}

```{r}
data("gss", package = "fciR")
gssrcc <- gss[, c("trump", "gthsedu", "magthsedu", "white", "female", "gt65")]
gssrcc <- gssrcc[complete.cases(gssrcc), ]
```





```{r}
message("This takes about 15 sec.. Load from file.")
# startTime <- Sys.time()
# gssrcc.out <- fciR::backdr_out(gssrcc,
#                         formula = trump ~ gthsedu + magthsedu + white + female + gt65)
# endTime <- Sys.time()
# print(endTime - startTime)
a_file <- file.path(dir_data, "chap06_gssrcc_out.rds")
# saveRDS(gssrcc.out, file = a_file)
gssrcc.out <- readRDS(file = a_file)
# gssrcc.out
```

and we compare with the author's

```{r}
comp <- data.frame(
  name = c("EY0", "EY1", "RD", "RR"),
  auth = c(0.233, 0.271, 0.038, 1.164),
  est = gssrcc.out$est[gssrcc.out$name %in% c("EY0", "EY1", "RD", "RR")]
)
stopifnot(sum(abs(comp$auth - comp$est)) < 0.01)
```

and the results are presented in table 6.8

```{r}
fciR::gt_measures(gssrcc.out, 
            title = "Table 6.8", 
            subtitle = paste("General Social Survey",
            "Outcome-model Standardization",
            "Effect of <em>More than High School Education</em> on <em>
            Voting for Trump</em>",
            sep = "<br>"))
```

## Standardization via Exposure Modeling

> The exposure model is also known as the *propensity score*, denoted $e(H)$,
as it is a function of $H$.

$$
\begin{align*}
e(H) = (T \mid H) = expit(\alpha_0 + \alpha_1 H_1 + \ldots + \alpha_k H_k)
\end{align*}
$$

the proof of

$$
E(Y(1)) = E \left( \frac{TY}{e(H)} \right)
$$
is


$$
\begin{align*}
&\text{by definition of expectation} \\
E \left( \frac{t \cdot y}{e(H)} \right) &= \sum_{y,t,h} \frac{TY}{e(H)} P(Y=y,T=t,H=h) \\
&\text{by multiplication rule} \\
&= \sum_{y,t,h} \frac{t \cdot y}{e(H)} P(Y=y \mid T=t,H=h) P(T=t \mid H=h) P(H=h) \\
&\text{because } T \text{ is binary, and by definition of } e(H) \text{ then } e(H) = P(T \mid H) \\ &= \sum_{y,t,h} \frac{t \cdot y}{e(H)} P(Y=y \mid T=t,H=h) e(H) P(H=h) \\
&\text{and when } T=0 \text{ the summand is zero, therefore we are left with } T=1 \\
&= \sum_{y,h} \frac{y}{e(H)} P(Y=y \mid T=1,H=h) e(H) P(H=h) \\
&\text{we cancel the } e(H) \text{ in numerator and denominator} \\
&= \sum_{y,h} y P(Y=y \mid T=1,H=h) P(H=h) \\
&\text{by definition of conditional expectation} \\
&= E_H (E(Y \mid T=1, H)) \\
&\text{and by (6.2) which implies (6.1)} \\
&= E(Y(1))
\end{align*}
$$




### Examples {-}

#### Mortality Rates by Country {-}

See section 1.2.1 in chapter 1 for details on `data_mortability_exp`.


```{r}
data("mortality_long", package = "fciR")
mortdat <- mortality_long
```


Compute the standardized estimates using exposure modeling with `fciR::backdr_exp_np`


```{r}
mortdat.out <- fciR::backdr_exp_bb(mortdat, formula = Y ~ `T` + H, weights = "n")
mortdat.out[c("EY0", "EY1")]
# verify with the author's
stopifnot(abs(mortdat.out$EY0 - 0.0078399) < 1e-7,
          abs(mortdat.out$EY1 - 0.0069952) < 1e-7)
```

```{r}
summ <- mort %>%
  count(Y, `T`, H, name = "n") %>%
  mutate(freq = n / sum(n))
summ
stopifnot(sum(summ$freq) == 1)

eH <- summ %>%
  group_by(`T`, H) %>%
  summarize(n = sum(n)) %>%
  group_by(H) %>%
  mutate(prob = n / sum(n)) %>%
  filter(`T` == 1) %>%
  arrange(H) %>%
  pull(prob)
# eH
stopifnot(all(eH > .Machine$double.eps^0.5))
eH0 <- eH[1]
eH1 <- eH[2]

is_att <- TRUE
e0 <- 1L
if (is_att) {
  e0 <- summ %>%
  filter(`T` == 1) %>%
  summarize(sum(freq)) %>%
  pull()
}

# create the  eH variable
EY <- summ %>%
  mutate(eH = (1 - H) * eH0 + H * eH1)

# compute the summand of the estimating equations
if (!is_att) {
  EY <- EY %>%
    mutate(s = (1 - `T`) * Y / (1 - eH) + `T` * Y / eH)
} else {
  EY <- EY %>%
    mutate(s = (1 - `T`) * Y * eH / (e0 * (1 - eH)) + `T` * Y / eH)
  # E(Y|T=1) is estimated as before (see very last paragraph of section 6.2.1)
  EYT <- summ %>%
    filter(`T` == 1) %>%
    group_by(Y) %>%
    summarize(n = sum(n)) %>%
    mutate(prob = n / sum(n)) %>%
    summarize(EYT = sum(Y * prob))
}
  

# Estimate the value of the potential outcome
EY <- EY %>%
  group_by(`T`) %>%
  summarize(EY = sum(s * freq)) %>%
  arrange(`T`) %>%
  pull(EY)

if (is_att) EY[2] <- EYT

EY
```




### Average Effect of Treatment on the Treated

It can be proven that

$$
E(Y(0) \mid T=1) = E \left( \frac{Y(1 - T) e(H)}{e_0(1 - e(H))}  \right), \, e_0 = P(T=1) \\
$$

as follows

$$
\begin{align*}
&\text{by the rule of double expectation} \\
E(Y(0) \mid T=1) &= E_{H \mid T=1} E(Y \mid T=0, H) \\
&\text{by definition of expectation} \\
&= E_{H \mid T=1} \left[ \sum_{y} y P(Y=y \mid T=0, H) \right] \\
&\text{by definition of conditional expectation} \\
&= \sum_h \left[ \sum_{y} y P(Y=y \mid T=0, H) \right] P(H=h \mid T=1) \\
&\text{by definition of conditional expectation we have that} \\ 
&P(H=h \mid T=1) = \frac{P(T=1 \mid H=h) P(H=h)}{P(T=1)} \\
&\text{therefore} \\
E(Y(0) \mid T=1) &= \sum_{y,h} y P(Y=y \mid T=0, H=h) \frac{P(T=1 \mid H=h) P(H=h)}{P(T=1)} \\
&\text{rearranging terms} \\
&= \sum_{y,h} y \frac{P(T=1 \mid H=h)}{P(T=1)} \left[ P(Y=y \mid T=0, H=h)P(H=h)  \right] \\
&\text{and multiply by } 1 = \frac{P(T=0 \mid H=h)}{P(T=0 \mid H=h)} \\
&= \sum_{y,h} y \frac{P(T=1 \mid H=h)}{P(T=1)} \left[ \frac{P(Y=y \mid T=0, H=h)P(T=0 \mid H=h)P(H=h)}{P(T=0 \mid H=h)}  \right] \\
&\text{rearranging the terms again} \\
&= \sum_{y,h} y \frac{P(T=1 \mid H=h)}{P(T=1)P(T=0 \mid H=h)} \left[ P(Y=y \mid T=0, H=h)P(T=0 \mid H=h)P(H=h)  \right] \\
&\text{using the multiplication rule} \\
&= \sum_{y,h} y \frac{P(T=1 \mid H=h)}{P(T=1)P(T=0 \mid H=h)} P(Y=y, T=0, H=h) \\
&\text{ and since } e(h) = P(T=1 \mid H=h) \text{ and } e_0 = P(T=1) \\
&= \sum_{y,h} y \cdot \frac{e(h)}{e_0 (1 - e(h))} \cdot P(Y=y, T=0, H=h) \\
&\text{ and since } \sum_t (1-t) P(Y=y, T=t, H=h) = P(Y=y, T=0, H=h) \\
&= \sum_{y,h} y \cdot \frac{e(h)}{e_0 (1 - e(h))} \cdot \sum_t (1-t) P(Y=y, T=t, H=h) \\
&= \sum_{y,h, t} y \cdot (1-t) \cdot \frac{e(h)}{e_0 (1 - e(h))} \cdot P(Y=y, T=t, H=h) \\
&\text{and by definition of expectation} \\
&= E \left[ Y \cdot (1-T) \cdot \frac{e(H)}{e_0 (1 - e(H))} \right]
\end{align*}
$$

and by the definition of conditional expectation we know that

$$

$$

See previous section for calculation with mortality data for the function with
the flag `is_att = TRUE`

```{r}
mortdat.out[["EY0T1"]]
stopifnot(abs(mortdat.out$EY0T1 - 0.010176) < 1e-6)
```


### Standardization with a Parametric Exposure Model


The function `fciR::backdr_exp()` is used to standardized with a parametric exposure
model and the `glm` fit. It is the main function used in the chapter.

Alternatively the standardization could be done with `geeglm` from the `geepack`
package. For *those focused primarily on the risk difference*. See the 
explanation on section 6.2.2 on why `geeglm` is not really good for the
risk ratio.

The function is called `exp` in the book. We rename it `fciR::backdr_exp()` to be
more informative and avoid mix up with the much-used base R function `exp.`



#### What-if? Study {-}

First we do it using the `glm` fit

```{r}
whatif2.exp <- fciR::backdr_exp(whatif2dat, formula = vl4 ~ A + lvlcont0)
# whatif2.exp
```

and compare with the author's


```{r}
comp <- data.frame(
  name = c("EY0", "EY1", "RD", "RR"),
  auth = c(0.36, 0.30, -0.06, 0.834),
  est = whatif2.exp$est[whatif2.exp$name %in% c("EY0", "EY1", "RD", "RR")])
stopifnot(sum(abs(comp$auth - comp$est)) < 0.01)
```



and the results are presented in table 6.9

```{r}
fciR::gt_measures(whatif2.exp, 
            title = "Table 6.9", 
            subtitle = paste("What-If Study", 
                             "Exposure-model Standardization with <em>H = lvlcont0</em>",
                             sep = "<br>"))
```


then we use the `geeglm` from the `geepack` package fit for risk difference

```{r}
message("This takes about 15 sec.. Load from file.")
# startTime <- Sys.time()
# whatif2.expgee <- fciR::backdr_exp_gee(whatif2dat, formula = vl4 ~ A + lvlcont0)
# endTime <- Sys.time()
# print(endTime - startTime)

a_file <- file.path(dir_data, "chap06_whatif2gee.rds")
# saveRDS(whatif2.expgee, file = a_file)
whatif2.expgee <- readRDS(a_file)

# we don't use the risk ratio measures with this function
whatif2.expgee <- whatif2.expgee[!(whatif2.expgee$name %in% c("RR*", "OR")), ]
# whatif2.expgee
```

and the results are presented in table 6.9

```{r}
fciR::gt_measures(whatif2.expgee, 
            title = "Table 6.9 geeglm", 
            subtitle = paste("What-If Study", 
                             "Exposure-model Standardization using <em>geeglm</em> wtih <em>H = lvlcont0</em>",
                             sep = "<br>"))
```


#### General Social Survey {-}

The `gssrcc` is defined in section 6.1.2 above. It is the `gss` data with
complete cases only.

The `standexp` function on page 119-120 of section 6.2.2 is not needed anymore
as `standexp` was created with parameters in the previous section. We just need
to run it as follows



```{r}
message("This takes about 10 sec. Load from file.")
# startTime <- Sys.time()
# gssrcc.exp <- fciR::backdr_exp(gssrcc,
#                         formula = trump ~ gthsedu + magthsedu + white + female + gt65)
# endTime <- Sys.time()
# print(endTime - startTime)
a_file <- file.path(dir_data, "chap06_gssrcc_exp.rds")
# saveRDS(gssrcc.exp, file = a_file)
gssrcc.exp <- readRDS(file = a_file)
# gssrcc.exp
```

and compare with the author's


```{r}
comp <- data.frame(
  name = c("EY0", "EY1", "RD", "RR"),
  auth = c(0.231, 0.272, 0.041, 1.176),
  est = gssrcc.exp$est[gssrcc.exp$name %in% c("EY0", "EY1", "RD", "RR")])
stopifnot(sum(abs(comp$auth - comp$est)) < 0.01)
```


and the results are presented in table 6.9

```{r}
fciR::gt_measures(gssrcc.exp, 
            title = "Table 6.10", 
            subtitle = paste(
              "General Social Survey<br>Exposure-model Standardization", 
              "Effect of <em>More than High School Education</em> on <em>
              Voting for Trump</em>",
            sep = "<br>"))
```


## Doubly Robust Standardization

The function `backdr_dr()` does a doubly robust standardization. It is not in the
text but is actually used for the exercise. It is very similar
to `badstanddr`.

The function `badstanddr` is replaced by `backdr_dr_bad`, used for doubly 
robust standardization with a misspecified outcome model.


and using the What-if Study we obtain


```{r}
whatif2.dr <- fciR::backdr_dr_bad(whatif2dat, formula = vl4 ~ A + lvlcont0)
# whatif2.dr
```

and compare with the author's


```{r}
comp <- data.frame(
  name = c("EY0", "EY1", "RD", "RR"),
  auth = c(0.362, 0.300, -0.062, 0.830),
  est = whatif2.dr$est[whatif2.dr$name %in% c("EY0", "EY1", "RD", "RR")])
stopifnot(sum(abs(comp$auth - comp$est)) < 0.01)
```



and the results are presented in table 6.9

```{r}
fciR::gt_measures(whatif2.dr, 
            title = "Table 6.12", 
            subtitle = paste("What-If Study", 
            "Doubly Robust Standardization",
            "Combining the Misspecified Outome Model of Table 6.11", 
            "and the Exposure Model of Table 6.9",
            sep = "<br>"))
```

### Doubly Robust Standardization Simulation

#### With `simdr`

The simulation of doubly robust standardization discussed at the end of 
section 6.3 in p. 126 to 130 and found in `simdr` is analyzed in an appendix at 
[Doubly Robust Simulation](#mc_standdr).

The results obtained by Brumback are close enough to what we have below.
Here is a tableau of her results



```{r echo=FALSE}
data(fciR::fci_tbl_06_13)
df <- fci_tbl_06_13

df <- df %>% select(ss, estimator, description, mean, sd, pval) %>%
  mutate(ss = paste("ss", ss, sep = "=")) %>%
  pivot_longer(cols = c("mean", "sd", "pval"), names_to = "stats",
               values_to = "value") %>%
  mutate(value = ifelse(stats == "pval", round(value, 2), round(value, 4))) %>%
  unite(col = "heading", ss, stats, sep = "_") %>%
  pivot_wider(id_cols = c("estimator", "description"), names_from = "heading",
              values_from = "value")


title <- "Table 6.13 and 6.14"
subtitle <- paste("Sampling Distribution from Simulation", 
                   "Investigating Small-Sample Robustness", 
                   "True E(Y(0))=0.01, True E(Y(1))=0.02",
                  sep = "<br>")
fciR::gt_standdr(df, title = title, subtitle = subtitle)
```

#### With `mc_standdr`

We perform the simulation using a Monte Carlo simulation called `mc_standdr`.
The script is in the appendix at [mc_standdr](#mc_standdr).

We use a sample size of only 1000 as in the book.

```{r}
nrep <- 1000
```

So here the simulation with $ss \in \{40, 100\}$

```{r}
message("This takes about 8 min. Load from file.")
# startTime <- Sys.time()
# mc.out <- fciR::mc_standdr(ss = c(40, 100), nrep = nrep)
# endTime <- Sys.time()
# print(endTime - startTime)
a_file <- file.path(dir_data, "chap06_mc_out.rds")
mc.out <- readRDS(file = a_file)
# saveRDS(mc.out, file = a_file)
```

and we compute the p-values

```{r}
mc.out <- mc.out %>%
    mutate(`T` = ifelse(grepl(pattern = "0", estimator), 0, 1),
         h0 = ifelse(`T` == 0, 0.01, 0.02),
         sdp = sd / sqrt(n),
         z = abs((mean - h0) / sdp),
         pval = 2 * (1 - pnorm(z))) %>%
  select(-sdp, -z)
# mc.out
```

and show the results in a table

```{r echo=FALSE}
the_estimators <- c("EYT0" = "Unadjusted", "EYT1" = "Unadjusted",
                      "EY0exp" = "Linear Exposure", "EY1exp" = "Linear Exposure",
                      "EY0exp2" = "Logistic Exposure", "EY1exp2" = "Logistic Exposure",
                      "EY0out" = "Overspecified Outcome", "EY1out" = "Overspecified Outcome",
                      "EY0dr" = "Doubly Robust", "EY1dr" = "Doubly Robust")
dft <- mc.out %>%
  select(ss, estimator, mean, sd, pval) %>%
  mutate(ss = paste("ss", ss, sep = "=")) %>%
  pivot_longer(cols = c("mean", "sd", "pval"), names_to = "stats", 
               values_to = "value") %>%
  mutate(value = ifelse(stats == "pval", round(value, 2), round(value, 4))) %>%
  unite(col = "heading", ss, stats, sep = "_") %>%
  pivot_wider(id_cols = "estimator", names_from = "heading", 
              values_from = "value") %>%
  mutate(description = the_estimators[match(estimator, names(the_estimators))]) %>%
  relocate(description, .after = estimator)
# reorder the rows to match book's
dft <- dft[match(names(the_estimators), dft$estimator), ]

title <- "Table 6.13 and 6.14 <em>(by FL)</em>"
subtitle <- paste("Sampling Distribution from Simulation", 
                   "Investigating Small-Sample Robustness", 
                   "True E(Y(0))=0.01, True E(Y(1))=0.02",
                  sep = "<br>")
fciR::gt_standdr(dft, title = title, subtitle = subtitle)
```

#### Plotting the Monte Carlo Simulation

We will not reiterate the comments from Brumback as the results in the tableau 
just above confirm them.

A plot can however illustrate Brumback's main points. This ones shows the
estimates' mean with their 5% and 95% quantiles from the simulation.

```{r echo=FALSE}
mc.out %>%
  select(ss, estimator, mean, lower, upper) %>%
  mutate(ss = paste("ss", ss, sep = "=")) %>%
  ggplot(aes(x = mean, xmin = lower, xmax = upper, y = estimator, color = ss)) +
  geom_pointrange(position = position_dodge(width = 0.5)) +
  geom_vline(xintercept = c(0.01, 0.02), color = c("darkgreen", "darkorange"),
             linetype = "dashed", size = 1) + 
  ggrepel::geom_text_repel(aes(x = mean, y = estimator, label = round(mean, 2)), 
                           size = 3) +
  ggrepel::geom_text_repel(aes(x = lower, y = estimator, label = round(lower, 2)), 
                           size = 3) +
  ggrepel::geom_text_repel(aes(x = upper, y = estimator, label = round(upper, 2)), 
                           size = 3) +
  scale_x_continuous(breaks = seq(from = -0.1, to = 0.1, by = 0.01)) +
  theme_minimal() +
  theme(title = element_text(color = "midnightblue"),
        legend.position = "bottom",
        legend.title = element_blank()) +
  labs(title = "Chap 6, section 6.3: Simulation of Standardization Methods",
       subtitle =
         sprintf("The mean with 2.5%% and 97.5%% quantiles. True E(Y(0)) = %.2f, True E(Y(1)) = %.2f.", 
                 0.01, 0.02),
       x = NULL, y = NULL)
```


## Exercises

The exercises are located in a separate project.
