[["probability.html", "Chapter 2 Conditional Probability and Expectation 2.1 Conditional Probability 2.2 Conditional Expectation and the Law of Total expectation 2.3 Estimation 2.4 Sampling Distributions and the Bootstrap 2.5 Exercises", " Chapter 2 Conditional Probability and Expectation library(tidyr) library(dplyr) library(purrr) library(rsample) library(fciR) options(dplyr.summarise.inform = FALSE) 2.1 Conditional Probability 2.1.1 Law of total probability It is important to note that \\(\\sum_i{H_i} = H\\), that is \\(H\\) can be partitioned in \\(i\\) non-overlapping partitions. Then the law of total probabilities is \\[ \\begin{align*} P(A) &amp;= \\sum_i{P(A \\cap H_i)}= \\sum_i{P(A \\mid H_i) P(H_i)} \\\\ &amp;\\text{and we condition the whole expression with B} \\\\ P(A \\mid B) &amp;= \\sum_i{P(A \\cap H_i \\mid B)}= \\sum_i{P(A \\mid B, H_i) P(B,H_i)} \\\\ \\end{align*} \\] and the multiplication rule is \\[ \\begin{align*} P(A, B \\mid C) &amp;= \\frac{P(A, B, C)}{P(C)} \\\\ &amp;= \\frac{P(A \\mid B, C) P(B, C)}{P(C)} \\\\ &amp;= \\frac{P(A \\mid B, C) P(B \\mid C) P(C)}{P(C)} \\\\ &amp;= P(A \\mid B, C) P(B \\mid C) \\end{align*} \\] 2.2 Conditional Expectation and the Law of Total expectation The conditional expectation is defined as \\[ E(Y \\mid T) = \\sum_y y P(Y=y \\mid T) \\] and one way that helps me usually understand it well is that conditioning is the same as filtering the data. For example the conditional expectation of mortality of US resident at the beginning of 2019 \\(E(Y \\mid T = 1) = 0.0088\\) data(&quot;mortality_long&quot;, package = &quot;fciR&quot;) mortality_long |&gt; # condition on T = 1 filter(`T` == 1) |&gt; # compute probabilties mutate(prob = n / sum(n)) |&gt; # compute expectation for each possible value of Y group_by(Y) |&gt; summarize(EYT1 = sum(Y * prob)) |&gt; # output results in a named vector pull() |&gt; setNames(nm = c(&quot;EY0T1&quot;, &quot;EY1T1&quot;)) ## EY0T1 EY1T1 ## 0.0000 0.0088 where \\(E(Y=0 \\mid T=1) = 0\\) because when \\(Y=0 \\implies 0 \\cdot P(Y=0) = 0\\) and since \\(Y\\) is binary \\(E(Y=1 \\mid T=1) = P(Y=1 \\mid T=1)\\). Analogous to the law of total probability is the law of ttal expectation, also called double expectation theorem. This law is used extensively in this textbook. \\[ \\begin{align*} E(Y \\mid T) &amp;= E_{H \\mid T}(E(Y \\mid H, T)) \\\\ &amp;= \\sum_h \\left[ \\sum_y y P(Y=y \\mid H=h, T) \\right] P(H=h \\mid T) \\end{align*} \\] Also the following equivalence is used very often in this book. \\[ \\begin{align*} E(H \\mid T) = \\sum_h h P(H=h \\mid T) = E_{H \\mid T} (H) \\end{align*} \\] 2.2.1 Mean independence and conditional mean independence The random variable \\(Y\\) is mean independent of \\(T\\) if \\[ E(Y \\mid T) = E(Y) \\] and is conditionally mean independent of \\(T\\) given \\(H\\) is \\[ E(Y \\mid T, H) = E(Y \\mid H) \\] and the conditional uncorrelation and uncorrelation are \\[ E(YT \\mid H) = E(Y \\mid H)E(T \\mid H) \\implies \\text{conditionally uncorrelated} \\\\ E(YT) = E(Y)E(T) \\implies \\text{uncorrelated} \\] It happens that conditional mean independence implies conditional uncorrelation, but not the other way around. We prove it as follows \\[ \\begin{align*} \\text{assume Y is conditionally independent of T given H then} \\\\ E(Y \\mid T, H) &amp;= E(Y \\mid H) \\\\ \\\\ \\text{using double expectation theorem} \\\\ E(TY \\mid H) &amp;= E_{T \\mid H}(E(TY \\mid H, T)) \\\\ \\text{expectation is a linear operator} \\\\ &amp;= E_{T \\mid H}(TE(Y \\mid H, T)) \\\\ \\text{by conditional mean independence from above} \\\\ &amp;= E_{T \\mid H}(TE(Y \\mid H)) \\\\ \\text{expectation is a linear operator} \\\\ &amp;= E_{T \\mid H}(T)E_{T \\mid H}(E(Y \\mid H)) \\\\ \\text{which proves the conditional uncorrelation} \\\\ &amp;= E(T \\mid H)E(Y \\mid H) \\end{align*} \\] 2.2.2 Regression model A statistical model for a conditional expectation is called a regression model. For a binary dataset the regression model is said to be saturated or nonparametric because the 4 proportions, or coefficients, cover all possibilities. \\[ E(Y \\mid T, H) = \\beta_0 + \\beta_1 H + \\beta_2 T + \\beta_3 H * T \\] When unsaturated or parametric the model makes an assumption. For example the following model assumes no interaction. \\[ E(Y \\mid T, H) = \\beta_0 + \\beta_1 H + \\beta_2 T \\] 2.2.3 Nonlinear parametric models The three parametric models, also the most well-known, used in the book are \\[ \\begin{align*} \\text{linear: } \\: E(Y \\mid X_1, \\ldots, X_p) &amp;= \\beta_0 + \\beta_1 X_1 + \\ldots + \\beta_p X_p \\\\ \\text{loglinear: } \\: E(Y \\mid X_1, \\ldots, X_p) &amp;= \\exp{(\\beta_0 + \\beta_1 X_1 + \\ldots + \\beta_p X_p)} \\\\ \\text{logistic: } \\: E(Y \\mid X_1, \\ldots, X_p) &amp;= \\text{expit}(\\beta_0 + \\beta_1 X_1 + \\ldots + \\beta_p X_p) \\end{align*} \\] The function expit() used by the author is actually the same as gtools::inv.logit(), boot::inv.logit() or stats::plogis(). In this project we use stats::plogis() to minimize dependencies since it is in base R. 2.3 Estimation This section is extremely important as it is used extensively, especially from the chapter 6 on. Lets use the What-if example to illustrate the mathematics of it. In that case, we have 3 covariates, \\(T\\) for naltrexone, \\(A\\) for reduced drinking and \\(H\\) for unsuppressed viral load. In the data set we have 165 inobservations, therefore \\(i=165\\). Let \\(X_i\\) denote the collection of \\(X_{ij}\\) for \\(j=1, \\ldots, p\\) where \\(p\\) is the number of covariates which is 3 in the What-if dataset. \\(X_i\\) is a horizontal vector. So for the What-if study \\[ X_i = \\begin{bmatrix}A_i &amp; T_i &amp; H_1 \\end{bmatrix} \\] and \\(\\beta\\) is a vertical vector. \\[ \\beta = \\begin{bmatrix}\\beta_1 \\\\ \\beta_2 \\\\ \\beta_3 \\end{bmatrix} \\] therefore \\[ \\begin{align*} X_i \\beta &amp;= \\begin{bmatrix}A_i &amp; T_i &amp; H_1 \\end{bmatrix} \\times \\begin{bmatrix}\\beta_1 \\\\ \\beta_2 \\\\ \\beta_3 \\end{bmatrix} \\\\ &amp;= \\beta_1 A_i + \\beta_2 T_i + \\beta_3 H_i \\end{align*} \\] we also have \\[ \\begin{align*} X_i^T (Y_i - X_i \\beta) &amp;= \\begin{bmatrix}A_i \\\\ T_i \\\\ H_1 \\end{bmatrix}(Y_i - \\beta_1 A_i + \\beta_2 T_i + \\beta_3 H_i) \\\\ &amp;= \\begin{bmatrix} A_i (Y_i - \\beta_1 A_i + \\beta_2 T_i + \\beta_3 H_i) \\\\ T_i (Y_i - \\beta_1 A_i + \\beta_2 T_i + \\beta_3 H_i) \\\\ H_1 (Y_i - \\beta_1 A_i + \\beta_2 T_i + \\beta_3 H_i) \\end{bmatrix} \\end{align*} \\] Also, we can estimate the unconditional expectation of the binary outcome \\(Y\\) as \\[ \\hat{E}(Y) = \\frac{1}{n} \\sum_{i=1}^n Y_i \\] which returns the proportion of participants with unsuppressed viral load after 4 months data(&quot;whatifdat&quot;, package = &quot;fciR&quot;) mean(whatifdat$Y) ## [1] 0.3212121 This is an unbiased estimator for \\(E(Y)\\) because \\(E(\\hat{E}(Y)) = E(Y)\\) Now let the saturated model for \\(E(Y)\\) be \\[ E(Y) = \\beta \\] with the following estimating equation \\[ U(\\beta) = \\sum_{i=1}^n (Y_i - \\beta) = 0 \\] and simple algebra, using the estimate of \\(\\hat{E}(Y)\\) from above gives \\(\\beta = \\hat{E}(Y)\\) and we use the notation \\(\\hat{E}(Y) = E(Y \\mid X, \\beta)\\) to show its dependency on the data and \\(\\beta\\). \\[ \\begin{align*} U(\\beta) &amp;= \\sum_{i=1}^n (Y_i - \\beta) \\\\ &amp;= \\sum_{i=1}^n \\left[ Y_i - E(Y \\mid X, \\beta) \\right] \\\\ &amp;= 0 \\end{align*} \\] and to do the actual calculations we use a variation of the estimating equation as follows \\[ \\begin{align*} U(\\beta) = \\sum_{i=1}^n X_i^T \\left[ Y_i - E(Y \\mid X, \\beta) \\right] = 0 \\end{align*} \\] As an example, we fit the logistic regression model with the What-if dataset \\[ E(Y \\mid A,T,H) = expit(\\beta_0 + \\beta_A A + \\beta_T T + \\beta_H H) \\] whatif.mod &lt;- glm(Y ~ A + `T` + H, family = &quot;binomial&quot;, data = whatifdat) summary(whatif.mod) ## ## Call: ## glm(formula = Y ~ A + T + H, family = &quot;binomial&quot;, data = whatifdat) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.7199 -0.5664 -0.4834 0.7932 2.1944 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -1.5234 0.4125 -3.693 0.000222 *** ## A -0.5647 0.4214 -1.340 0.180248 ## T -0.2254 0.4147 -0.543 0.586790 ## H 2.7438 0.4158 6.600 4.12e-11 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 207.17 on 164 degrees of freedom ## Residual deviance: 150.83 on 161 degrees of freedom ## AIC: 158.83 ## ## Number of Fisher Scoring iterations: 4 The coefficients, \\(\\beta_0, \\beta_A, \\beta_T, \\beta_H\\) are obtained with coef() which is an alias for coefficient(). The example in the book uses lmod$coef which is not a recommended coding practice. The data should be obtained with an extractor function such as coef(). coef(whatif.mod) ## (Intercept) A T H ## -1.5233530 -0.5647068 -0.2254112 2.7438245 and since the model is binomial then the inverse function to convert from the logit scale to the natural scale is stats::plogis(). Therefore the parametric estimate is \\[ E(Y \\mid A=1, T=1, H=1) = \\beta_0 + \\beta_A \\cdot 1 + \\beta_T \\cdot 1 + \\beta_H \\cdot 1 \\] stats::plogis(sum(coef(whatif.mod))) ## [1] 0.6059581 we can also use the linear model directly, thus avoiding a call to plogis and making the code a little more robust. This technique is used quite often in the book, especially starting from chapter 6 on. whatif.newdata &lt;- whatifdat |&gt; filter(A == 1, `T` == 1, H == 1) whatif.EYT1 &lt;- predict(whatif.mod, newdata = whatif.newdata, type = &quot;response&quot;) |&gt; mean() whatif.EYT1 ## [1] 0.6059581 and we can validate the result directly using the dataset whatifdat.est &lt;- whatifdat |&gt; filter(A == 1, `T` == 1, H == 1) |&gt; pull(Y) |&gt; mean() whatifdat.est ## [1] 0.5909091 which is close to the parametric estimate. 2.4 Sampling Distributions and the Bootstrap The previous section prived and estimate. But how variable is this estimator? One way would be to report the standard error \\(\\sigma_{\\hat{x}} = \\frac{\\sigma}{\\sqrt{n}}\\). Therefore, since \\(\\sigma^2 = p(1-p)\\) for a binomial distribution # the sample size whatifdat.n &lt;- sum(whatifdat$A == 1 &amp; whatifdat$`T` == 1 &amp; whatifdat$H == 1) # whatifdat.n # the standard deviation whatifdat.sd &lt;- sqrt(whatifdat.est * (1- whatifdat.est)) # whatifdat.sd # the standard error whatifdat.se &lt;- whatifdat.sd / sqrt(whatifdat.n) whatifdat.se ## [1] 0.1048236 Note that the formula used by B. Brumback is using the standard deviation of the population \\(p(1-p)\\) which is not entirely the right way since the standard error of the mean is defined using the standard deviation of the sample. That is Brumback uses x &lt;- whatifdat$Y[whatifdat$A == 1 &amp; whatifdat$`T` == 1 &amp; whatifdat$H ==1] n &lt;- length(x) sd(x) * sqrt(n-1) / n ## [1] 0.1048236 when the correct definition is x &lt;- whatifdat$Y[whatifdat$A == 1 &amp; whatifdat$`T` == 1 &amp; whatifdat$H ==1] sd(x) / sqrt(length(x)) ## [1] 0.1072903 Another way would be by reporting the sampling distribution of our estimator. the sampling distribution is centered at the true value of our estimand and it has a standard deviation equal to the true standard error of our estimator. The section 2.4 will not be repeated here, yet it is very instructive and cover a topic that we end to forget. We will just repeat the calculations below. They The sim() function in section 2.4, p. 31 is coded in fciR::sim_intervals() and its alias fciR::sim(). Run and verify with authors results on p. 32 d &lt;- fciR::sim_intervals(nsim = 500, n = 500) stopifnot(abs(d$bad - 0.8374) &lt; 0.03, abs(d$good - 0.948) &lt; 0.03) d ## $bad ## [1] 0.86 ## ## $good ## [1] 0.948 2.4.1 Bootstrapping As we have seen, replacing \\(\\mu\\) and \\(\\sigma\\) with estimates leads to some problems, but we can still form an interpretable 95% confidence interval. In many situations, we do not have a good estimate of \\(\\sigma\\). In those cases, we often can turn to bootstrap. Bootstrapping in base R is done with the boot from the boot package. Another option is the rsample package which isthe tidyverse way of doing it. Both methods will be used in this project. In the package fciR, the functions boot_run and boot_est use the classic R package boot. The functions bootidy_run and bootidy_est are similar but use the rsample package. 2.4.1.1 boot package We use boot::boot() for boostrapping and boot::boot.ci() to compute the confidence interval of the estimand using the logistic model as done by the lmodboot function in section 2.4. We only run 100 boot samples since it is only an example. The default value is 1000. The first 3 arguments data, statistic and R are specific to boot, the ... is for extra arguments used by lmodboot. The details of lmdboots can be found in the package fciR or in its alias fciR::prob_lmod. prob_lmod(whatifdat, formula = Y ~ `T` + A + H) ## logitP ## 0.4303535 and we show the details on how to do the bootstrapping with the boot package as follows. # boot_ci &lt;- function(x, alpha = 0.05) { # # critical value using t distribution # cv &lt;- qt(1 - alpha, df = length(x) - 1) # # mean value # est &lt;- mean(x) # sd &lt;- sd(x) # c(&quot;est&quot; = mean(conf), conf = conf, lci = est - cv * sd, uci = est + cv * sd) # } # define the function used by boot whatifdat.fnc &lt;- function(data, ids, formula = Y ~ `T` + A + H) { dat &lt;- data[ids, ] prob_lmod(data = dat, formula = formula) } # run the bootstrapping whatifdat.boot &lt;- boot::boot(data = whatifdat, statistic = whatifdat.fnc, R = 100, formula = Y ~ `T` + A + H) # get the confidence interval using the boostrap object sapply(X = seq_along(whatifdat.boot$t0), FUN = function(i, alpha = 0.05, method = &quot;norm&quot;) { est &lt;- whatifdat.boot$t0[i] ci &lt;- boot::boot.ci(whatifdat.boot, conf = 1 - alpha, type = method, index = i)$normal # ci &lt;- as.data.frame(ci) # out &lt;- c(est, ci) data.frame( &quot;term&quot; = &quot;P&quot;, &quot;.lower&quot; = plogis(ci[2]), &quot;.estimate&quot; = plogis(est), &quot;.upper&quot; = plogis(ci[3]), &quot;.alpha&quot; = alpha, &quot;.method&quot; = method) # ci # names(out) &lt;- c(&quot;est&quot;, &quot;conf&quot;, &quot;lci&quot;, &quot;uci&quot;) # plogis(out[c(&quot;est&quot;, &quot;lci&quot;, &quot;uci&quot;)]) }) |&gt; t() |&gt; as.data.frame() ## term .lower .estimate .upper .alpha .method ## 1 P 0.4190503 0.6059581 0.7753866 0.05 norm 2.4.1.2 rsample package Another way package that could be used for boostrapping is rsample::bootstrap which uses the tidyverse way of R programming. To be fully tidyverse-like we recode the prob_lmod function from above and call it prob_lmod_td where the td suffix stands for tidyverse. Note the use of a formula as an argument to the function and the rlang package which is a great tool to learn for quality code in R. prob_lmod_td &lt;- function(data, formula = Y ~ `T` + A + H, condition.names = NULL) { # independent variables from the formula f_vars &lt;- all.vars(rlang::f_rhs(formula)) # if condition.names is NULL then use all independent variables # which is the same as saying there is no condition if (is.null(condition.names)) condition.names &lt;- f_vars stopifnot(all(condition.names %in% f_vars)) # add intercept to conditions x0 &lt;- &quot;(Intercept)&quot; # name of intercept used by lm, glm, etc. condition.names &lt;- c(x0, condition.names) fit &lt;- glm(formula = formula, family = &quot;binomial&quot;, data = data) |&gt; tidy() fit |&gt; filter(term %in% condition.names) |&gt; summarize(term = &quot;logitP&quot;, estimate = sum(estimate), # don&#39;t know the std.err so no t-intervals std.err = NA_real_) } prob_lmod_td(whatifdat, formula = Y ~ `T` + A + H) ## # A tibble: 1 x 3 ## term estimate std.err ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 logitP 0.430 NA The following function is inspired from rsample bootstrap. We obtain the confidence interval using rsample::int_pctl which uses a percentile interval instead of assuming a normal distribution as Ms Brumback does. The result are not materially different in the current case. whatifdat |&gt; rsample::bootstraps(times = 100, apparent = FALSE) |&gt; mutate(results = map_dfr(splits, function(x) { dat &lt;- analysis(x) prob_lmod_td(dat, formula = Y ~ `T` + A + H) })) |&gt; rsample::int_pctl(results, alpha = 0.05) |&gt; mutate(term = &quot;P&quot;, .lower = plogis(.lower), .estimate = plogis(.estimate), .upper = plogis(.upper)) ## Warning: Recommend at least 1000 non-missing bootstrap resamples for term ## `logitP`. ## # A tibble: 1 x 6 ## term .lower .estimate .upper .alpha .method ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 P 0.434 0.607 0.751 0.05 percentile and lmodboot() is fciR::boot_lmod() with the alias fciR::lmodboot() Finally we run fciR::boot_lmod() and verify against the authors results on p.34 data(&quot;whatifdat&quot;, package = &quot;fciR&quot;) message(&quot;this takes 1 sec.: we load a saved file instead&quot;) ## this takes 1 sec.: we load a saved file instead # startTime &lt;- Sys.time() # out &lt;- fciR::boot_est(whatifdat, func = fciR::prob_lmod, R = 500, conf = 0.95, # inv = &quot;expit&quot;, evars = &quot;logit&quot;, formula = Y ~ `T` + A + H) # endTime &lt;- Sys.time() # print(endTime - startTime) a_file &lt;- file.path(dir_data, &quot;chap02_lmodboot.rds&quot;) # saveRDS(out, file = a_file) out &lt;- readRDS(file = a_file) # out # check results with book stopifnot(abs(out[&quot;est&quot;] - 0.60596) &lt; 0.02, abs(out[&quot;lci&quot;] - 0.41638) &lt; 0.02, abs(out[&quot;uci&quot;] - 0.76823) &lt; 0.02) 2.5 Exercises The exercises are located in a separate project. "]]
