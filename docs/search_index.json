[["index.html", "Study project: undamentals of Causal Inferences With R Study Project About Packages", " Study project: undamentals of Causal Inferences With R François Lefebvre 2022-02-01 Study Project About This is a study project of Brumback (2022). Many thanks to Babette Brumback for a great book to tackle the question of causality. It is extremely in my professional life which is not about making predictions but rather come up with intervention plans (business intel). The suggestions for errata are in the section Errata. Comments are also included in the section Comments. Packages The functions have been rewritten to simplify them and improve the learning experience (personal opinion) for newbies such as me. The following packages are so useful that they could not be avoided. In particular, dplyr and tidyr make the coding experience so much more interesting that one could almost claim they have become the standard in R coding. dplyr for data wrangling. see Wickham, François, et al. (2021) tidyr for data wrangling. see Wickham (2021) ggplot2 for plots, see Wickham, Chang, et al. (2021)` gt for all tables, see Iannone, Cheng, and Schloerke (2021) dagitty for analysis of structural causal models, see Textor, van der Zander, and Ankan (2021) ggdag for directed acyclic graphs, see Barrett (2021) gee: a generalized estimation equation solver. Introduced in chapter 4. See Carey, Lumley, and Ripley (2019). MonteCarlo: To perform Monte Carlo simulations. Introduced in chapter 4, section 4.2. See Leschinski (2019). simstudy: To run all sorts of simulations. A great tool to learn right from the start since simulations are so important. See Goldfeld and Wujciak-Jens (2021). References "],["errata.html", "Errata Preface Chapter 1 Chapter 2 Chapter 3", " Errata Preface page xi, last word of first paragraph is standaridzation, s/b standardization Chapter 1 Section 1.2.3.2, p. 11 The sentence of the 6th line on top of the page is We simulated the data according to the hyothetical, s/b hypothetical Chapter 2 Figure 2.1, p. 30 This is really a small detail. The caption of the bottom plot is \\(\\hat{E_{np}}(Y \\mid A= 1, H =1, T = 1)\\), s/b \\(\\hat{E}_{np}\\) Chapter 3 Typography: section 3.2 p. 40, equation 3.1 The current latex expression of conditional independence used seems to be (Y(0), Y(1)) \\ \\text{II} \\ T with the output \\[ (Y(0), Y(1)) \\ \\text{II} \\ T \\] a better typography would be \\perp\\!\\!\\!\\perp for the symbol \\(\\perp\\!\\!\\!\\perp\\). When used for equation 3.1 as (Y(0), Y(1)) \\perp\\!\\!\\!\\perp T we obtain \\[ (Y(0), Y(1)) \\perp\\!\\!\\!\\perp T \\] In the case when we want to show dependence, that is no independence then the latex expression is \\not\\!\\perp\\!\\!\\!\\perp for the symbol \\(\\not\\!\\perp\\!\\!\\!\\perp\\). For example equation 3.1 would become \\[ (Y(0), Y(1)) \\not\\!\\perp\\!\\!\\!\\perp T \\] "],["comments.html", "Comments Chapter 2 Chapter 4 Chapter 5", " Comments Chapter 2 section 2.4 p. 31 The second sentence of the last paragraph on p. 33 says We also need the car package in order for the summary() function to operate on boot objects the way we describe. This sentence is not required if we use the boot::boot.ci() which simplifies lmodboot.r() and does not require the car package. See the code in this document for lmodboot.r in chapter 2. Chapter 4 Section 4.1 See the plots in section 4.2. They could be helpful to visualize the changes in effect measures from one level of modifier to the other. Section 4.2 Monte Carlo Simulation A Monte Carlo is provided in section 4.2 and coded in a function called betasim_effect_measures(). It uses the \\(Beta\\) distribution. It is helpful in that it confirms the same results as in Jake Shannin (2021) is less CPU intensive as it needs only 5000 iterations to confirm Jake Shannin (2021) is easier to code than java and uses R which is the declared language of Brumback (2022) allows some extra flexibility with the shape parameters of \\(Beta\\) to investigate the conclusion with diffferent curves. See the suggestion for applications below. page 72, Figure 4.1 The probabilites shown in the Venn diagram do not add up to 100% because, for example, the event that RR changes in the same direction as RD but not in the same direction as the other two measures []. It would akward to arbitrarily one of those 2 chances as zero. Jake Shannin (2021) mentions that it is the result of not mutually exclusive events. That is true. Yet, these events, properly grouped are actually mutually exclusive. In section 4.2 they are called Opposite pairwise events. Using these definitions then yes, they are mutually exclusive but cannot be properly shown in the Venn diagram. This can be easily solved by splitting the probabilities. See section 4.2 for details. The end result a proper partitioning of the sample space \\(\\Omega\\) and is, in fact, a \\(\\sigma-field\\) (See Geoffrey R. Grimmet (2001), section 1.2). Yet it does not change the conclusions reached in Jake Shannin (2021). Actually, it reinforces them as this point is extremely important when using probabilities and statistics. Applications See my sub-section 4.2 called Applications where 2 possible applications are mentioned. Data pre-processing (data cleaning) Bayesian prior for Beta-binomial model Exercises Exercise 1 Using the causal power, the conclusion is different than the official answer. It is not obvious why the official solution does not make use of the causal power. Exercise 5 The official solution uses gee with the default family, that is gaussian. Since the outcome \\(attend\\) is binary isnt it better to use the binomial family? We quote p. 50 from chapter 3 in that respect Because our outcome is binary, we choose to fit the logistic parametric model Chapter 5 The dagitty and ggdag are used extensively. library(ggplot2, quietly = TRUE) library(ggdag, quietly = TRUE) References "],["intro.html", "Chapter 1 Introduction 1.1 A Brief History 1.2 Data Examples 1.3 Exercises", " Chapter 1 Introduction 1.1 A Brief History 1.2 Data Examples 1.2.1 Mortality Rates by Country 1.2.2 National Center for Education Statistics 1.2.3 Reducing Alcohol Consumption 1.2.3.1 The What-If? Study 1.2.3.1.1 The Double What-If? Study The code can be found in the file doublewhatifsim.R and is verified against the authors file just below. The DAG for the Double What-If? study in the dagitty version is scm &lt;- list() scm &lt;- within(scm, { the_nodes &lt;- c(&quot;U&quot; = &quot;Unmeasured, healthy behavior (U=1)&quot;, &quot;AD0&quot; = &quot;Adherence time 0&quot;, &quot;VL0&quot; = &quot;Viral Load time 0&quot;, &quot;T&quot; = &quot;Naltrexone (T=1)&quot;, &quot;A&quot; = &quot;Reduced drinking (A=1)&quot;, &quot;AD1&quot; = &quot;Adherence time 1&quot;, &quot;VL1&quot; = &quot;Viral Load time 1&quot;) coords &lt;- data.frame( name = names(the_nodes), x = c(2, 3, 4, 1, 2, 3, 4), y = c(2, 2, 2, 1, 1, 1, 1) ) dag &lt;- dagify( AD0 ~ U, VL0 ~ AD0, A ~ `T` + U, AD1 ~ A, VL1 ~ AD0 + AD1 + U, outcome = &quot;VL1&quot;, exposure = &quot;T&quot;, latent = &quot;U&quot;, coords = coords, labels = the_nodes) # status&#39; colors colrs &lt;- c(&quot;latent&quot; = &quot;palevioletred&quot;, &quot;exposure&quot; = &quot;mediumspringgreen&quot;, &quot;outcome&quot; = &quot;cornflowerblue&quot;) # plot the DAG plot &lt;- dag %&gt;% tidy_dagitty() %&gt;% ggdag_status(color = status, text = TRUE, node_size = 10, text_size = 3, text_col = &quot;black&quot;) + scale_color_manual(values = colrs, na.value = &quot;honeydew3&quot;) + scale_fill_manual(values = colrs, na.value = &quot;honeydew3&quot;) + ggdag::theme_dag_blank(panel.background = element_rect(fill=&quot;snow&quot;, color=&quot;snow&quot;)) + theme(legend.position = &quot;bottom&quot;, legend.title = element_blank()) + labs(title = &quot;The Double What-If? Study&quot;) }) scm$plot and the code for doublewhatifsim.R is doublewhatifsim &lt;- function(n = 1000, seed = 444) { set.seed(seed) # variables each with probability 0.5 U &lt;- rbinom(n, size = 1, prob = 0.5) # probability of AD0 depends on U AD0prob &lt;- 0.2 + 0.6 * U # generate independent bernoulli variables with varying probabilities AD0 &lt;- rbinom(n, size = 1, prob = AD0prob) VL0prob &lt;- 0.8 - 0.4 * AD0 VL0 &lt;- rbinom(n, size = 1, prob = VL0prob) `T` &lt;- rbinom(n, size = 1, prob = 0.5) Aprob &lt;- 0.05 + `T` * U * 0.8 A &lt;- rbinom(n, size = 1, prob = Aprob) AD1prob &lt;- 0.1 + 0.8 * A AD1 &lt;- rbinom(n, size = 1, prob = AD1prob) VL1prob &lt;- VL0prob + 0.1 - 0.45 * AD1 VL1 &lt;- rbinom(n, size =1 , prob = VL1prob) data.frame( &quot;AD0&quot; = AD0, &quot;VL0&quot; = VL0, &quot;T&quot; = `T`, &quot;A&quot; = A, &quot;AD1&quot; = AD1, &quot;VL1&quot; = VL1 ) } which we verify against the raw data set provided by the author. bb &lt;- doublewhatifdat d &lt;- doublewhatifsim() stopifnot(identical(d, bb)) 1.2.4 General Social Survey 1.2.5 A Cancer Clinical Trial 1.3 Exercises Please note that the solutions to exercises of Brumback (2022) below are my own and have not been verified or approved in any way by the author. 1.3.1 Exercise 1 8 Experiment 1.3.2 Exercise 2 9 Analogy 1.3.3 Exercise 3 2 Consistency: They are well known to cause substantial weight gain. That is, observed by different persons, different places and time. 6 Plausability 7 Coherence 1.3.4 Exercise 4 8 Experiment 9 Analogy 1.3.5 Exercise 5 3 Specificity 1.3.6 Exercise 6 5 Biological gradient: increasing levels of physical activity are more strongly protective 8 Experiment 1.3.7 Exercise 7 6 Plausability 4 Temporality 1.3.8 Exercise 8 4 Temporality library(dplyr, quietly = TRUE) library(boot, quietly = TRUE) References "],["probability.html", "Chapter 2 Conditional Probability and Expectation 2.1 Conditional Probability 2.2 Conditional Expectation and he Law of Total expectation 2.3 Estimation 2.4 Sampling Distributions and the Bootstrap 2.5 Exercises", " Chapter 2 Conditional Probability and Expectation The mortality data is dataMortality &lt;- data.frame( &quot;T&quot; = c(TRUE, TRUE, FALSE, FALSE), &quot;H&quot; = c(FALSE, TRUE, FALSE, TRUE), &quot;deaths&quot; = c(756340, 2152660, 2923480, 7517520), &quot;population&quot; = c(282305227, 48262955, 1297258493, 133015479), &quot;Y&quot; = c(0.002679, 0.044603, 0.002254, 0.056516) ) stopifnot(near(sum(dataMortality$Y), sum(dataMortality$deaths / dataMortality$population), tol = 1e-6)) 2.1 Conditional Probability 2.1.1 Law of total probability It is important to note that \\(\\sum_i{H_i} = H\\), that is \\(H\\) can be partioned in \\(i\\) non-overlapping partitions. Then the law of total probabilities is \\[ \\begin{align*} P(A) &amp;= \\sum_i{P(A \\cap H_i)}= \\sum_i{P(A \\mid H_i) P(H_i)} \\\\ &amp;\\text{and we condition the whole expression with B} \\\\ P(A \\mid B) &amp;= \\sum_i{P(A \\cap H_i \\mid B)}= \\sum_i{P(A \\mid B, H_i) P(B,H_i)} \\\\ \\end{align*} \\] and the multiplication rule is \\[ \\begin{align*} P(A, B \\mid C) &amp;= \\frac{P(A, B, C)}{P(C)} \\\\ &amp;= \\frac{P(A \\mid B, C) P(B, C)}{P(C)} \\\\ &amp;= \\frac{P(A \\mid B, C) P(B \\mid C) P(C)}{P(C)} \\\\ &amp;= P(A \\mid B, C) P(B \\mid C) \\end{align*} \\] 2.2 Conditional Expectation and he Law of Total expectation The function expit() used by the author is actually the same as gtools::inv.logit() or stats::plogis(). In this project we use stats::plogis() because it is in base R and we want to minimize dependencies. 2.3 Estimation Using the What-if example we have \\[ \\begin{align*} X_i \\beta &amp;= X_{i,1} \\beta_i +\\ldots+X_{i, p}\\beta_p \\\\ &amp;\\therefore \\\\ X_i \\beta &amp;= \\beta_1 A_i + \\beta_2 T_i + \\beta_3 H_i \\\\ \\end{align*} \\] and \\[ X_i^T(Y_i - X_i \\beta) \\\\ \\therefore \\\\ \\begin{bmatrix} 1 \\cdot(Y_i - \\beta_1 - A_i \\beta_2 - T_i \\beta_3 - H_i \\beta_4) \\\\ A_i \\cdot(Y_i - \\beta_1 - A_i \\beta_2 - T_i \\beta_3 - H_i \\beta_4) \\\\ T_i \\cdot(Y_i - \\beta_1 - A_i \\beta_2 - T_i \\beta_3 - H_i \\beta_4)\\\\ H_i \\cdot(Y_i - \\beta_1 - A_i \\beta_2 - T_i \\beta_3 - H_i \\beta_4) \\end{bmatrix} \\] 2.4 Sampling Distributions and the Bootstrap This code reflects better coding practices than the textbooks I think. In particular with the use of replicate() and avoiding the for loop. sim.r &lt;- function(nsim = 1e4, n = 1e4) { # get the statistics from the 2 samplings out &lt;- replicate(n = nsim, expr = { y &lt;- rnorm(n) x &lt;- rnorm(n) muhaty &lt;- mean(y) # mean of the y sample sehaty &lt;- sqrt(var(y) / n) # standard error of the y sample lci &lt;- muhaty - 1.96 * sehaty # lower bound of the y sample uci &lt;- muhaty + 1.96 * sehaty # upper bound of the y sample muhatx &lt;- mean(x) # mean of the x sample # is mean(x) within the interval from y? bad &lt;- (muhatx &gt;= lci) &amp; (muhatx &lt;= uci) # is the true mean of 0 within the interval from y? good &lt;- (0 &gt;= lci) &amp; (0 &lt;= uci) # return the results c(&quot;bad&quot; = bad, &quot;good&quot; = good) }) # convert to data.frame out &lt;- data.frame(t(out)) pbad &lt;- sum(out$bad) / nrow(out) # proportion in bad interval pgood &lt;- sum(out$good) / nrow(out) # proportion in good interval # return results in a list list(&quot;bad&quot; = pbad, &quot;good&quot; = pgood) } Run and verify with authors results on p. 32 d &lt;- sim.r() d ## $bad ## [1] 0.8318 ## ## $good ## [1] 0.9515 stopifnot(near(d$bad, 0.8374, tol = 0.01), near(d$good, 0.948, tol = 0.01)) The textbooks version of this function has 13 lines. This one only has 7 and use boot::boot.ci() and stats::plogis() to simplify. Also we change the way the function works to harmonize it with other similar functions coming up in later chapters use formulas in the parameters to better understand what the process is. This point is important as this is, in my experience, the tricky part to understand. # - Estimate the unconditional sampling distribution when using all # variables with cond = all.vars(formula[[3]]). # - It is the conditional sampling distribution when using only some predictors # such as with Y ~ `T` + A + H as is done in exercise 5 below. lmodboot.r &lt;- function(dat, formula = Y ~ `T` + A + H, cond = Y ~ `T` + A + H, R = 1000, conf = 0.95) { # the name of the intercept variable used by glm x0 &lt;- &quot;(Intercept)&quot; # the name of the predictor variables in the condition formula cond &lt;- all.vars(cond[[3]]) estimator &lt;- function(data, ids) { dat &lt;- data[ids, ] coefs &lt;- coef(glm(formula = formula, family = binomial, data = dat)) # use cond to identify conditioned variables. # i.e. the user can decide not to use all variables from the formula # or, in other words, condition on some variables coefs &lt;- coefs[c(x0, cond)] sum(coefs) } boot.out &lt;- boot::boot(data = dat, statistic = estimator, R = R) # the estimate on the logit scale est.logit &lt;- boot.out$t0 # the confidence interval on the logit scale using normal interval ci.logit &lt;- boot::boot.ci(boot.out, conf = conf, type = &quot;norm&quot;)$normal # the estimate and confidence interval on the natural scale # NOTE: the plogis() function gives the inverse logit out &lt;- c(&quot;est&quot; = plogis(est.logit), &quot;conf&quot; = ci.logit[1], &quot;lci&quot; = plogis(ci.logit[2]), &quot;uci&quot; = plogis(ci.logit[3])) out } Run and verify against the authors results on p.34 d &lt;- lmodboot.r(whatifdat) d ## est conf lci uci ## 0.6059581 0.9500000 0.4147740 0.7652328 # check results with book stopifnot(near(d[&quot;est&quot;], 0.60596, 0.01), near(d[&quot;lci&quot;], 0.41638, 0.01), near(d[&quot;uci&quot;], 0.76823, 0.015)) 2.5 Exercises Please note that the solutions to exercises of Brumback (2022) below are my own and have not been verified or approved in any way by the author. 2.5.1 Exercise 1 df &lt;- whatifdat probs &lt;- list() # probability of having A = 1 and T = 1 probs[&quot;B,C&quot;] &lt;- nrow(df[df$A == 1 &amp; df$`T` == 1, ]) / nrow(df) # conditioning on C means conditioning on T = 1 which means # that we have to filter the data with the condition T = 1 dfC &lt;- df[df$`T` == 1, ] # the probability of C, i.e. T = 1 probs[&quot;C&quot;] &lt;- nrow(dfC) / nrow(df) # the prob of B given C (i.e. nb of row with A = 1 using filtered data) probs[&quot;B|C&quot;] &lt;- nrow(dfC[dfC$A == 1, ]) / nrow(dfC) # check the result stopifnot(near(probs[[&quot;B,C&quot;]], probs[[&quot;B|C&quot;]] * probs[[&quot;C&quot;]])) msg &lt;- sprintf(&quot;prob(B,C) = %0.6f, prob(B|C) x prob(C) = %0.6f&quot;, probs[[&quot;B,C&quot;]], probs[[&quot;B|C&quot;]] * probs[[&quot;C&quot;]]) message(msg) ## prob(B,C) = 0.315152, prob(B|C) x prob(C) = 0.315152 2.5.2 Exercise 2 We create the datasets to reflect the conditioning, i.e. the filtering based on the conditions given. df &lt;- nces dfAC &lt;- df[df$highmathsat == 1 &amp; df$selective == 1, ] dfBC &lt;- df[df$female == 1 &amp; df$selective == 1, ] dfC &lt;- df[df$selective == 1, ] then we calculate the conditional probabilities probs &lt;- list() probs[&quot;A|B,C&quot;] &lt;- nrow(dfBC[dfBC$highmathsat == 1, ]) / nrow(dfBC) probs[&quot;B|A,C&quot;] &lt;- nrow(dfAC[dfAC$female == 1, ]) / nrow(dfAC) probs[&quot;A|C&quot;] &lt;- nrow(dfC[dfC$highmathsat == 1, ]) / nrow(dfC) probs[&quot;B|C&quot;] &lt;- nrow(dfC[dfC$female == 1, ]) / nrow(dfC) probs$x &lt;- probs[[&quot;B|A,C&quot;]] * probs[[&quot;A|C&quot;]] / probs[[&quot;B|C&quot;]] stopifnot(near(probs[[&quot;A|B,C&quot;]], probs$x)) msg &lt;- sprintf(&quot;%0.6f = %0.6f&quot;, probs[[&quot;A|B,C&quot;]], probs$x) message(msg) ## 0.345238 = 0.345238 2.5.3 Exercise 3 Mathematically \\[ \\begin{align*} P(A \\mid B) &amp;= \\frac{P(A,B)}{P(B)} \\\\ &amp;= \\frac{P(B \\mid A) P(A)}{P(B, A) + P(B, \\neg{A})} \\\\ &amp;= \\frac{P(B \\mid A) P(A)}{P(B \\mid A)P(A) + P(B \\mid \\neg{A})P(\\neg A)} \\end{align*} \\] Empirically df &lt;- whatifdat dfA &lt;- df[df$`T` == 1, ] dfAnot &lt;- df[df$`T` != 1, ] dfB &lt;- df[df$H == 1, ] probs &lt;- list() probs[&quot;A|B&quot;] &lt;- nrow(dfB[dfB$`T` == 1, ]) / nrow(dfB) probs[&quot;B|A&quot;] &lt;- nrow(dfA[dfA$H == 1, ]) / nrow(dfA) probs[&quot;B|-A&quot;] &lt;- nrow(dfAnot[dfAnot$H == 1, ]) / nrow(dfAnot) probs[&quot;A&quot;] &lt;- nrow(dfA) / nrow(df) probs[&quot;-A&quot;] &lt;- nrow(dfAnot) / nrow(df) probs$x &lt;- (probs[[&quot;B|A&quot;]] * probs[[&quot;A&quot;]]) / (probs[[&quot;B|A&quot;]] * probs[[&quot;A&quot;]] + probs[[&quot;B|-A&quot;]] * probs[[&quot;-A&quot;]]) stopifnot(near(probs$x, probs[[&quot;A|B&quot;]])) msg &lt;- sprintf(&quot;%0.6f = %0.6f&quot;, probs[[&quot;A|B&quot;]], probs$x) message(msg) ## 0.542373 = 0.542373 2.5.4 Exercise 4 See section 2.2, top of p. 23, for comment that conditional mean independence implies unconditional uncorrelation, but not the other way around. First, we see that the unconditional correlation is demonstrated. \\[ \\begin{align*} E(XY) &amp;= \\sum_x \\sum_y x \\cdot y \\cdot P(X = x, Y = y) \\\\ &amp;= 1 \\cdot 1 \\cdot P(X = 1, Y = 1) + 2 \\cdot -2 \\cdot P(X = 2, Y = -2) + 3 \\cdot 1 \\cdot P(X = 3, Y = 1) \\\\ &amp;= 1 \\cdot 1 \\cdot \\frac{1}{3} + 2 \\cdot -2 \\cdot \\frac{1}{3} + 3 \\cdot 1 \\cdot \\frac{1}{3} = 0 \\\\ \\\\ E(X)E(Y) &amp;= \\sum_x \\sum_y x \\cdot P(X = x, Y = y) \\times \\sum_x \\sum_y y \\cdot P(X = x, Y = y) \\\\ &amp;= \\left( 1 \\cdot \\frac{1}{3} + 2 \\cdot \\frac{1}{3} + 3 \\cdot \\frac{1}{3} \\right) \\times \\left( 1 \\cdot \\frac{2}{3} + -2 \\cdot \\frac{1}{3} \\right) = 0 \\\\ \\\\ &amp;\\therefore \\\\ E(XY) &amp;= 0 = E(X)E(Y) \\end{align*} \\] and to show that we do not have a mean conditional independence, it is sufficient to show for any value \\(x\\) that \\(E(Y \\mid X = x) \\neq E(Y)\\) \\[ \\begin{align*} E(Y \\mid X = 1) &amp;= \\sum_y y P(Y = y \\mid X = 1) \\\\ &amp;= \\sum_y y \\frac{P(X = 1, Y = y)}{P(X = 1)} \\\\ &amp;= 1 \\cdot \\frac{P(X = 1, Y = 1)}{P(X = 1)} -2 \\cdot \\frac{P(X = 1, Y = -2)}{P(X = 1)} \\\\ &amp;= 1 \\cdot \\frac{\\frac{1}{3}}{\\frac{1}{3}} - 2 \\cdot \\frac{0}{\\frac{1}{3}} = 1 \\\\ \\\\ E(Y) &amp;= \\sum_y y \\cdot P(Y= y) \\\\ &amp;= 1 \\cdot \\frac{2}{3} - 2 \\cdot \\frac{1}{3} = 0 \\\\ \\\\ &amp;\\therefore \\\\ E(Y \\mid X = 1) &amp;\\neq E(Y) \\end{align*} \\] 2.5.5 Exercise 5 See section 2.4 at the end on how to do this, p. 34 and 35. The nonparametric estimate and ci is df &lt;- whatifdat df &lt;- df[(df$A == 1) &amp; (df$H == 0) &amp; (df$`T` == 1), ] nonparametric &lt;- list( est = mean(df$Y), se = sqrt(var(df$Y) / nrow(df)) ) nonparametric &lt;- within(nonparametric, { lci &lt;- est - 1.96 * se uci &lt;- est + 1.96 * se }) nonparametric ## $est ## [1] 0.1 ## ## $se ## [1] 0.0557086 ## ## $uci ## [1] 0.2091889 ## ## $lci ## [1] -0.009188859 and using the bootstrap function from above we get the parametric estimate and ci parametric &lt;- lmodboot.r(whatifdat, formula = Y ~ `T` + A + H, cond = Y ~ `T` + A) parametric ## est conf lci uci ## 0.09001343 0.95000000 0.04382705 0.21463501 which is very similar to the nonparametric and a little narrower because of the bias-variance tradeoff as discussed in the last paragraph of chapter 2. References "],["outcomes.html", "Chapter 3 Potential Outcomes and the Fundamental Problem of Causal Inference", " Chapter 3 Potential Outcomes and the Fundamental Problem of Causal Inference "],["measures.html", "Chapter 4 Effect-Measure Modification and Causal Interaction", " Chapter 4 Effect-Measure Modification and Causal Interaction "],["dag.html", "Chapter 5 Causal Directed Acyclic Graphs", " Chapter 5 Causal Directed Acyclic Graphs "],["backdoor.html", "Chapter 6 Backdoor Method vis Standardization", " Chapter 6 Backdoor Method vis Standardization "],["references.html", "References", " References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
