# Effect-Measure Modification and Causal Interaction {#measures}

```{r echo=TRUE, message=FALSE, warning=FALSE}
library(dplyr, quietly = TRUE)
library(tidyr, quietly = TRUE)
library(gt, quietly = TRUE)
library(ggplot2, quietly = TRUE)
library(boot, quietly = TRUE)
library(gee, quietly = TRUE)
library(MonteCarlo, quietly = TRUE)
library(ggvenn, quietly = TRUE)
```



```{r include=FALSE}
# the directory of documentation for chapter 1
dir_docs01 <- file.path(dirname(getwd()), "FundamentalsCausalInference_docs",
                   "Brumback FOCI Website Material", "Chapter 1")
# the directory of documentation for chapter 3
dir_docs03 <- file.path(dirname(getwd()), "FundamentalsCausalInference_docs",
                   "Brumback FOCI Website Material", "Chapter 3")
# the directory of documentation for chapter 4
dir_docs04 <- file.path(dirname(getwd()), "FundamentalsCausalInference_docs",
                   "Brumback FOCI Website Material", "Chapter 4")
# directory of data files
dir_data <- file.path(getwd(), "data")
# directory for functions
dir_lib <- file.path(getwd(), "lib")
```


```{r message=FALSE, warning=FALSE, include=FALSE}
source(file = file.path(dir_lib, "boot_utils.R"), 
       local = knitr::knit_global())
source(file = file.path(dir_lib, "gt_utils.R"), 
       local = knitr::knit_global())
source(file = file.path(dir_lib, "fci_04-A_boot.R"), 
       local = knitr::knit_global())
source(file = file.path(dir_lib, "gt_measures_modif.R"),
       local = knitr::knit_global())
source(file = file.path(dir_lib, "ggp_measures_modif.R"),
       local = knitr::knit_global())
source(file = file.path(dir_lib, "mc_beta_effect_measures.R"),
       local = knitr::knit_global())
source(file = file.path(dir_lib, "ggp_venn_sim.R"),
       local = knitr::knit_global())
source(file = file.path(dir_lib, "ggp_betasim.R"),
       local = knitr::knit_global())
load(file.path(dir_docs01, "gss.RData"))
load(file.path(dir_docs03, "brfss.RData"))
load(file.path(dir_docs04, "sepsis.RData"))
load(file.path(dir_docs01, "nces.RData"))
```



## Effect-Measure Modification and Statistical Interaction

### RECOVERY trial


Create the data.frame for *RECOVERY* trial

```{r echo=TRUE}
recovery <- expand.grid(Y = 0:1, `T` = 0:1, M = 0:1)
recovery$n <- as.integer(c(787, 2851, 368, 1412, 278, 405, 86, 238))
recovery <- lapply(X = seq_len(nrow(recovery)), FUN = function(i) {
  data.frame(
    M = rep(recovery$M[i], recovery$n[i]),
    `T` = rep(recovery$`T`[i], recovery$n[i]),
    Y = rep(recovery$Y[i], recovery$n[i])
  )
})
recovery <- do.call(rbind, recovery)
# create an id variable for use with gee() later
recovery$id <- seq_len(nrow(recovery))
```



```{r eval=TRUE, echo=TRUE, file="lib\\fci_04-A_boot.R"}

```


run `boot()` with the RECOVERY data set

```{r}
recovery.out <- boot(recovery, formula = Y ~ `T` + M)
```


verify the results with the author's on p. 65.

```{r}
bb <- data.frame(
  name = c("EYT0.M0", "EYT0.M1", "EYT1.M0", "EYT1.M1", "RD.M0", "RD.M1",
           "EYT0.diff", "EYT1.diff", "RD.diff", "RR.M0", "RR.M1", "RR.diff",
           "RR*.M0", "RR*.M1", "RR*.diff", "OR.M0", "OR.M1", "OR.diff"),
  est = c(0.784, 0.593, 0.793, 0.735, 0.01, 0.142,
          -0.191, -0.059, 0.132, 1.012, 1.239, 1.224,
          1.046, 1.533, 1.466, 1.059, 1.9, 1.794)
  )
comp <- data.frame(bb = bb,
                   d = recovery.out[match(bb$name, recovery.out$name), "est"])
comp$dev <- abs(comp$bb.est - comp$d)
stopifnot(sum(comp$dev) < 0.01)
```

and we communicate the results in a table

```{r}
gt_measures_modif(recovery.out, title = "Table 4.2 RECOVERY Trial")
```





plotting the results makes it easier to see the measures vary among
the strata. We can clearly see here significant difference in effect measures
between the 2 strata.

It supports the observation in the text concerning the 
lack of effect of dexamethasone without intrusive mechanical ventilation (M0)
vs its use with intrusive mechanical ventilation (M1) which is significant.


```{r}
ggp_measures_modif(recovery.out, title = "RECOVERY trial")
```


The `gee::gee()` function is used to find information on the coefficients
and see if they are statistically significant.  The same could be done
withe `glm::glm()` but `gee` offers results with robust statistics which
is very useful in practical terms.

Luckily, the `gee()` works exactly like the `glm()` functions, with the same
extractor functions `coefficients()`, `effects()`, etc. See the documentation
of `lm` with `?lm` for more details.

We are going through an example just below to illustrate how tthe extractor 
functions are used which is not shown in the textbook.

```{r}
linmod <- gee::gee(Y ~ `T` + M + `T` * M,
                   id = id,
                   data = recovery,
                   family = gaussian)
summary(linmod)
```
To extract the coefficients from the `gee` object we use the extractor function
`coefficients()` or its alias `coef()`

```{r}
coef(linmod)
```

and to extract the entire coefficient data to work
with it, just use `coefficients()` with `summary()`

```{r}
coef(summary(linmod))
```

and in this case we are concerned about how significant the interaction is.
Therefore the *Robust z*  is extracted with `coefficients()` alias `coef()`

```{r}
coef(summary(linmod))[, "Robust z"]
```

we see that $T:M$ is 3.99 standard deviations away from zero which will give
us the 2-sided p-value that is significant

```{r}
z <- coef(summary(linmod))["T:M", "Robust z"]
2 * (1 - pnorm(z))
```

### NCES


We process the `NCES` data the same way we did for the RECOVERY trial.

Run the bootsrap


```{r}
nces.out <- boot(nces, formula = highmathsat ~ female + selective)
```

verify the results with the author's on p. 70.

```{r}
bb <- data.frame(
  name = c("EYT0.M0", "EYT0.M1", "EYT1.M0", "EYT1.M1", "RD.M0", "RD.M1",
           "EYT0.diff", "EYT1.diff", "RD.diff", "RR.M0", "RR.M1", "RR.diff",
           "RR*.M0", "RR*.M1", "RR*.diff", "OR.M0", "OR.M1", "OR.diff"),
  est = c(0.167, 0.675, 0.081, 0.345, -0.086, -0.33,
          0.509, 0.264, -0.244, 0.486, 0.511, 1.052,
          0.907, 0.496, 0.547, 0.44, 0.254, 0.576)
  )
comp <- data.frame(bb = bb, d = nces.out[match(bb$name, nces.out$name), "est"])
comp$dev <- abs(comp$bb.est - comp$d)
stopifnot(sum(comp$dev) < 0.01)
```


and the table is

```{r}
gt_measures_modif(nces.out, title = "Table 4.3 NCES data")
```

and we plot the results

```{r}
ggp_measures_modif(nces.out, title = "NCES data")
```


We observe that

* RD: Risk difference shows that using selection to accept more women
seem to decrease the % of school with hogh math SAT
* OR and RRstar: Show the same results as RD
* RR: Indicates that selection, in relative terms *has no significant effect*


## Qualitative Agreement of Effect Measures in Modification

This section relies heavily on the paper from Shannin and Brumback (2021) 
@shanninbrumback2021. It used a Monte-Carlo simulation
in java by [jake](https://github.com/gatorjake/EffectMeasures) running 1000000
times for six effect measures (the 4 in this chapter, the hazard ratio HR and
the recovery ration HR*).

For the purpose of this project we only simulate the 4 effect measures
discussed so far (RD, RR, RR* and OR). We use R package `MonteCarlo` with 5000
repetitions. The distribution used for simulation is the beta distribution 
which is generally used for values in [0,1]. It is also used as a prior of 
binomial regression in Bayes analysis which is the  subject covered a little 
later in this section. Regardless if the 6 effects measures from @shanninbrumback2021
or the 4 from @brumback2022 are used, the process and conclusion are the
same.

We point out that using the distribution $Beta(1, 1) \sim Uniform(0, 1)$ is 
equivalent to running a grid search. It is also equivalent to the uniform
distribution used in @shanninbrumback2021.


### Simulate the effect measures

We run the Monte-Carlo simulation without constraint and with $Beta(1,1)$ which
is equivalent to $Uniform(0,1)$ used by @shanninbrumback2021.

```{r}
message("this takes 1 sec.: we load a saved file instead")
startTime <- Sys.time()
# gridsim <- mc_beta_effect_measures(shape1 = 1, shape2 = 1, nrep = 5000)
a_file <- file.path(dir_data, "chap04_gridsim.rds")
# saveRDS(gridsim, file = a_file)
gridsim <- readRDS(file = a_file)
endTime <- Sys.time()
# print(endTime - startTime)
```

which gives the vector of percentages

```{r}
unlist(gridsim)
```

For the following discussion, we must note the following about the vector of
percentages returned by the simulation.

1. **Pairwise events**: Some measures move as *1 pair* in the same direction while 
all the other pairs move in different direction between each other.
These are 6 possibilities named `RD_RR`,`RD_RRstar`, `RD_OR`, `RR_RRstar`, `RR_OR`, `RRstar_OR`.
2. **Opposite pairwise events**: Some measures move as *2 pairs* but 
*each of the 2 pairs does not move in the same direction*. 
There are 3 possibilities, called `RD_RR_vs_RRstar_OR`, 
`RD_RRstar_vs_RR_OR` and `RD_OR_vs_RR_RRstar`. **These are the problematic ones 
as they cannot be represented in the Venn diagram of section 4.2**. However
we can distribute them to ensure probabilities add up to 1. For example
`RD_RR_vs_RRstar_OR` will be split 50% between pairwise events `RD_RR` and
50% to pairwise event`RRstar_OR`.  This enforces the very important rule 
that probabilities must add up to 1 without consequences on the conclusions
reached.
3. **3-wise events**: Some 3 measures move in the same direction together There are 4 possibilities
called `RD_RR_RRstar`, `RD_RR_OR`, `RD_RRstar_OR` and `RR_RRstar_OR`.
4. **All events**: Sometimes all measures move together. 
**This is the possibility of interest** discussed by Shannin and Brumback (2021). 
This possibility is called `ALL`.
5. **No event**: The possibility `NONE` concerns the event that no pair of 
measures move in the same direction. It is impossible and represents the empty set
$\emptyset$ which is one of the 3 conditions of a $\sigma-field$.
6. **Validation**: The sum of the vector's elements must be one.

The event definitions above ensure that the sample space is actually a $\sigma-field$.
See @grimmett, section 1.2.


then we compare with the author's results

```{r}
bb <- c("RD_RR" = 0.026, "RD_RRstar" = 0.026, "RD_OR" = 0, 
        "RR_RRstar" = 0, "RR_OR" = 0.026, "RRstar_OR" = 0.026,
        "RD_RR_vs_RRstar_OR" = 0,
        "RD_RRstar_vs_RR_OR" = 0,
        "RD_OR_vs_RR_RRstar" = 0,
        "RD_RR_RRstar" = 0, "RD_RR_OR" = 0.057, 
        "RD_RRstar_OR" = 0.057, "RR_RRstar_OR" = 0,
        "RD_RR_RRstar_OR" = 0.833, "NONE" = 0)
comp <- data.frame(bb = round(bb, 4), sim = round(unlist(gridsim), 4))
comp
```

The results from the Monte Carlo simulation above confirm the main conclusion
from @shanninbrumback2021 that all effect measures move together 84% of the time.

```{r}
c("simulation" = unname(unlist(gridsim["RD_RR_RRstar_OR"])), 
  "author's" = unname(bb["RD_RR_RRstar_OR"]))
```

We note that the sim adds up correctly to 1 but not the author's which adds up
to 1.051. This is explained on p. 72 as a bit of arbitrary allocations.  The
@shanninbrumback2021 paper (caption figure 1) mentions that they do not
add to 1 because they include events that *are not mutually exclusive*.

```{r}
c("simulation" = sum(unlist(gridsim)), "author's" = sum(bb))
```


Actually, this is caused by the **Opposite pairwise events** which cannot be 
represented in the 4-set Venn diagram. For the purpose of this
books we will simply split them between the events that makes them up without
consequence on the conclusion.

The **Opposite pairwise events** are the following

```{r}
# The events that pair of measures move together but in opposite
# direction of another pair who also move together
unlist(gridsim[c("RD_RR_vs_RRstar_OR", "RD_RRstar_vs_RR_OR", "RD_OR_vs_RR_RRstar")])
```

and when we split them between their 2 sub-events we can generate the
Venn diagram as follows


```{r}
ggp_venn_sim(gridsim, n = 1000, 
         fill_colr = c("blue", "yellow", "green", "red"),
         title = "Venn diagram of effect measure modifications")
```


now for **constrained data**

```{r}
message("this takes 1 sec.: we load a saved file instead")
startTime <- Sys.time()
# gridsim_const <- mc_beta_effect_measures(shape1 = 1, shape2 = 1, nrep = 5000,
#                                       constrained = TRUE)
a_file <- file.path(dir_data, "chap04_gridsim_const.rds")
# saveRDS(gridsim_const, file = a_file)
gridsim_const <- readRDS(file = a_file)
endTime <- Sys.time()
# print(endTime - startTime)
```

which gives the vector of percentages

```{r}
unlist(gridsim_const)
```

and comparing to the author's results


```{r}
bb_const <- c("RD_RR" = 0.053, "RD_RRstar" = 0.053, "RD_OR" = 0.053, 
        "RR_RRstar" = 0, "RR_OR" = 0.053, "RRstar_OR" = 0.0,
        "RD_RR_vs_RRstar_OR" = 0,
        "RD_RRstar_vs_RR_OR" = 0,
        "RD_OR_vs_RR_RRstar" = 0,
        "RD_RR_RRstar" = 0, "RD_RR_OR" = 0.114, 
        "RD_RRstar_OR" = 0.114, "RR_RRstar_OR" = 0,
        "RD_RR_RRstar_OR" = 0.667, "NONE" = 0)
sum(bb_const)
comp <- data.frame(bb = round(bb_const, 4), 
                   sim = round(unlist(gridsim_const), 4))
comp
```



The results from the Monte Carlo simulation with the constrained data 
agree the author's result.

```{r}
c("simulation" = unname(unlist(gridsim_const["RD_RR_RRstar_OR"])),
  "author's" = unname(bb_const["RD_RR_RRstar_OR"]))
```

Again the **Opposite pairwise events** cannot be represented
in the 4-set Venn diagram.

```{r}
# The events that pair of measures move together but in opposite
# direction of another pair who also move together
unlist(gridsim_const[c("RD_RR_vs_RRstar_OR", "RD_RRstar_vs_RR_OR", "RD_OR_vs_RR_RRstar")])
```

but if we do it as mentioned above then we can show a Venn diagram as follows

```{r}
# This is a custom function using the venn package
ggp_venn_sim(gridsim_const,
         fill_colr = c("cyan", "gold", "springgreen", "hotpink"),
         title = "Venn diagram for constrained data")
```


### Applications

#### Simulation of distribution of effect measures

We use a parametric Monte Carlo simulation using the beta distribution
to evaluate the effect of the distribution assumption on the effect-measure
modifications.

We run the Monte-Carlo simulation with 5000 repetitions, a grid of 
`shape1` and `shape2` parameters for the Beta distribution and no
constraint. Namely `mc_beta_effect_measures(shape1 = c(0.5, 1, 3, 5, 7),
shape2 = c(0.5, 1, 3, 5, 7), nrep = 5000)`

```{r}
message("this takes 25 sec.: we load a saved file instead")
startTime <- Sys.time()
# priorsim <- mc_beta_effect_measures(shape1 = c(0.5, 1, 3, 5, 7),
#                                   shape2 = c(0.5, 1, 3, 5, 7),
#                                   nrep = 5000)
a_file <- file.path(dir_data, "chap04_priorsim.rds")
# saveRDS(priorsim, file = a_file)
priorsim <- readRDS(file = a_file)
endTime <- Sys.time()
# print(endTime - startTime)
```

and we look at the matrix for the event `RD_RR_RRstar_OR` which represents the
event that all effect measures move in the same direction.

The matrix elements correspond to the percentage frequency of the event given
the beta distribution with shape parameters `shape1 = s1` with `s1` indicated
as column names, and `shape2 = s1` parameter with `s2` indicated in the row names.

The beta distribution with `shape1 = 1` and `shape2 = 1` is similar to the uniform
distribution on $[0, 1]$.  Therefore this element is the one simulated by Shannon
and Brumback, see @shanninbrumback2021 and, in fact, with 5000 repetitions 
`nrep = 5000`the measure is almost always very close to what is mentioned
in @shanninbrumback2021.

Also note that for large $shape1 = shape2 = \text{large number}$ the beta distribution
is similar to the normal distribution with a mean of $shape1 / (shape1 + shape2)$.
The simulation shows that in that case the percentage of measures moving in the
same directions is nearing 100%. See for example when $shape1 = shape2 = 7$ in
the matrix of results from the sim.

here is the matrix of percentage of times that all measures move in the same
direction, i.e. the event `RD_RR_RRstar_OR`

```{r}
round(priorsim[["RD_RR_RRstar_OR"]], 2)
```

and to show as a heatmap

```{r}
ggp_betasim(priorsim, var = "RD_RR_RRstar_OR", 
             colr = list("low" = "deepskyblue1", "high" = "deepskyblue4"))
```



and comparing with the author

```{r}
round(c("simulation" = priorsim[["RD_RR_RRstar_OR"]]["s2=1", "s1=1"], 
  "author's" = unname(bb["RD_RR_RRstar_OR"])), 3)
```

We can see that the range is wide and should be considered.

```{r}
range(priorsim[["RD_RR_RRstar_OR"]])
```

Now we do it with constrained data


```{r}
message("this takes 30 sec.: we load a saved file instead")
startTime <- Sys.time()
# priorsim_const <- mc_beta_effect_measures(shape1 = c(0.5, 1, 3, 5, 7),
#                                         shape2 = c(0.5, 1, 3, 5, 7),
#                                         nrep = 5000, constrained = TRUE)
a_file <- file.path(dir_data, "chap04_priorsim_const.rds")
# saveRDS(priorsim_const, file = a_file)
priorsim_const <- readRDS(file = a_file)
endTime <- Sys.time()
# print(endTime - startTime)
```

```{r}
ggp_betasim(priorsim_const, var = "RD_RR_RRstar_OR",
             colr = list("low" = "lightsalmon1", "high" = "lightsalmon4"),
             title = "Monte Carlo simulation. Constrained data.")
```


and comparing with the author

```{r}
round(c("simulation" = priorsim_const[["RD_RR_RRstar_OR"]]["s2=1", "s1=1"], 
  "author's" = unname(bb_const["RD_RR_RRstar_OR"])), 3)
```

and the range is even larger and therefore more significant.

```{r}
range(priorsim_const[["RD_RR_RRstar_OR"]])
```

#### Application: Data pre-processing (data cleaning)


Unless the $p_0, p_1$ obtained are uniformly distributed 
(not the most common scenario for sure) it seems from the
results above that the likelihood of *not* having all effect measures moving
in the same direction is low.  Thus, it could be a good hint to uncover hidden 
processes in a data pre-processing routine.

We note the important rule mentioned at the beginning of section 4.2

> when relative risk $RR$ and other relative risk $RR^*$ both change in the 
same direction [...] then so must the difference the risk difference and
odds ratio.

Another rule from section 4.2 can help in data cleaning

> When $RR_0$ and $RR_1$ are on opposite side of 1, that is, when in one stratum
the treatment is helpful and in the other it is harmful, then all measures 
will automatically change together.


Thus a quick, easy way to clean up potential data problems and obtain relevant
details on outliers and **hidden processes** might be


1. Exclude cases when $RR$ and $RR^*$ change in the same direction to reduce
the data load.
2. Exclude cases when $RR_0$ and $RR_1$ are on opposite side of 1
3. Investigate the remining cases asthey are good candidates for *hidden processes*


#### Application: Bayesian prior in Beta-binomial model

If we use information from the population, or from expert knowledge, 
that effect measures should move in the same direction then a beta distribution with $shape1 > 1, shape2 > 1$
would make sense and provide a better prior for the Beta-binomial model.

On the contrary, if we wish that the model look into the unlikely events
that effect measure do not move in the same directions, then 
a beta distribution with $shape1 \leq 1, shape2 \leq 1$ could be supported
by the results from above.

## Causal Interaction


## Exercises

Please note that the solutions to exercises of @brumback2022 below are my own 
and have not been verified or approved in any way by the author.


### Exercise 1


Comment: Using the causal power, the conclusion is different than the official 
answer. It is not obvious why the official solution does not make use of 
the *causal power*.


Let $Y \in \{0,1\}$ mean that there is a cost barrier $Y=1$ or not $Y=0$.  

Let $T \in \{0,1\}$ mean that the individual is a person with disability $T=1$
or not $T=0$  

Let $M \in \{0,1\}$ mean that the individual is 65-year or older $M = 1$ or
is younger than 65 year-old $M=0$.

and so we are given that

$$
p_0 = \hat{E}(Y \mid T = 0, M = 0) = 0.225 \\
p_1 = \hat{E}(Y \mid T = 1, M = 0) = 0.383 \\
p_2 = \hat{E}(Y \mid T = 0, M = 1) = 0.04 \\
p_3 = \hat{E}(Y \mid T = 1, M = 1) = 0.072 
$$

and so we compute the effect measures by age group

$$
\begin{align*}
RD_0 &= p_1 - p_0 = 0.383-0.225  \\
RD_1 &= p_3 - p_2 = 0.072-0.04 \\
RR_0 &= \frac{p_1}{p_0} = \frac{0.383}{0.225} \\
RR_1 &= \frac{p_3}{p_2} = \frac{0.072}{0.04} \\
RR^*_0 &= \frac{1-p_0}{1-p_1}=\frac{1-0.225}{1-0.383} \\
RR^*_1 &= \frac{1-p_2}{1-p_3}=\frac{1-0.04}{1-0.072} \\
RO_0 &= \frac{\frac{p_1}{1-p_1}}{\frac{p_0}{1-p_0}} \\
RO_1 &= \frac{\frac{p_3}{1-p_3}}{\frac{p_2}{1-p_2}}
\end{align*}
$$

which gives

```{r}
p0 <- 0.225
p1 <- 0.383
p2 <- 0.04
p3 <- 0.072
RD0 <- p1 - p0
RD1 <- p3 - p2
RR0 <- p1 / p0
RR1 <- p3 / p2
RR0star <- (1-p0) / (1-p1)
RR1star <- (1-p2) / (1-p3)
RO0 <- (p1 / (1-p1)) / (p0 / (1-p0))
RO1 <- (p3 / (1-p3)) / (p2 / (1-p2))
c("RD0" = RD0, "RD1" = RD1, "RR0" = RR0, "RR1" = RR1,
  "RR0*" = RR0star, "RR1*" = RR1star, "RO0" = RO0, "RO1" = RO1)
```

and if we define

$$
\begin{align*}
RD &= RD_1 - RD_0 \\
RR &= RR_1 - RR_0 \\
RR^* &= RR1_1^* - RR_0^* \\
RO &= RO_1 - RO_0
\end{align*}
$$

then

```{r}
RD = RD1 - RD0
RR = RR1 - RR0
RRstar = RR1star - RR0star
RO = RO1 - RO0
c("RD" = RD, "RR" = RR, "RR*" = RRstar, "RO" = RO)
```
*Is there a clear answer to determining which age group has a stronger effect
of disability having a cost barrier to health care?*

The difference in $RD$ from people 65-year older and younger ones is -0.126
but the difference in Risk ratio $RR$ is 0.098 and therefore not in the same 
direction which normally is a minority case and needs to investigated before
reaching a conclusion.  Thereofre, no, there is no clear answer to the question.

*Which age group would benefit the most from an intervention?*

The causal power (see p. 45) $CP_0$ represents the fraction of people who are
younger than 65 year-old who have a disability would not have met a cost barrier 
to health if they had received helped to compensate for their disabilities.


$$
CP_0 = \frac{p_1 - p_0}{1-p_0}= \frac{0.383-0.225}{1-0.225} = 0.2
$$

The causal power (see p. 45) $CP_1$ represents the fraction of people who are
older than 65 year-old who have a disability would not have met a cost barrier 
to health if they had received helped to compensate for their disabilities.


$$
CP_1 = \frac{p_3 - p_2}{1-p_2}= \frac{0.072-0.04}{1-0.04} = 0.3
$$

Therefore it seems that the help would benefit people older than 65 about 50% 
more than younger people. However we have to be careful since, as mentioned
above, not ll measures move in the same direction and further investigation is 
necessary.


### Exercise 2

We process the data with function `boot` from above.

```{r}
sepsis.out <- boot(sepsis, formula = zubrod45 ~ shock + gt65)
```

then create the table of results

```{r}
gt_measures_modif(sepsis.out, title = "Exercise 2: Sepsis Study")
```

and plotting the results

```{r}
ggp_measures_modif(sepsis.out, title = "Exercise 2: Sepsis Study")
```

Both $RR$ and $OR$ are show a reduction in relative terms when comparing 65-yars 
olds to younger people. That is, in relative terms, the event of a shock does 
reduces more the zubrod score for older people.

However the effect is not entirely clear as not all effect measures change
in the same direction as should be the case.  More investigaitons is required.


### Exercise 3

We process the data with function `boot` from above. See exercise #3 of
chapter 3 for details on the `brfss` data set. We are required to filter
respondents aged less than 65.

```{r}
dataBRFSS <- brfss[brfss$gt65 == 0, ]
```

we process with 500 boot iterations

```{r}
message("This takes 2 min.: We used a saved file.")
# startTime <- Sys.time()
# brfss.out <- boot(dataBRFSS, formula = insured ~ gthsedu + whitenh,
#                     R = 500, conf = 0.95)
# endTime <- Sys.time()
# print(endTime - startTime)
a_file <- file.path(dir_data, "chap04_ex3.rds")
# saveRDS(brfss.out, file = a_file)
brfss.out <- readRDS(file = a_file)
```

then create the table of results

```{r}
gt_measures_modif(brfss.out, title = "Exercise 3: BRFSS Survey")
```

and plotting the results

```{r}
ggp_measures_modif(brfss.out, title = "Exercise 3: BRFSS Survey")
```

For causal interpretaion we would need to assume that `whitenh` and `gthsedu`
are not correlated together by another latent variable which could possibly
be the case here.

if a causal interpretation were possible since all effect measures show
a reduction of the effect of having higher education when the respondent
is not white non-hispanic. In other words being white and nonhispanic
interacts with higher education to have an increased chance of being insured.


### Exercise 4


```{r}
dataWAVE <- data.frame(
  T1 = c(0, 0, 1, 1),
  T2 = c(0, 1, 0, 1),
  Y = c(0.019, 0.057, 0.039, 0.094),
  n = c(108, 105, 103, 107))
```


#### Using `glm`

To evaluate statistical significance we use the `glm()` function. Because our
outcome is numeric, we choose to fit the gaussian parametric model.
Also we use the `weights` parameter with the variable $n$ since the data is
aggregated.


```{r}
wave.glm <- glm(formula = Y ~ T1 + T2 + T1:T2, 
                family = gaussian, weights = n, data = dataWAVE)
summary(wave.glm)
```

And the `glm` function cannot compute because of the dispersion measure for
the `gaussian` family.

Therefore we use `gee` which uses robust statistics and saves the day.


#### Using `gee`

`gee` does not have a `weights` parameter like `glm` so we expand the data
before using `gee`.

```{r}
dfs <- vector(mode = "list", length = 4)
pos <- 1
dfs[[pos]] <- do.call(rbind, 
                      replicate(dataWAVE$n[pos], 
                                data.frame(T1 = 0, T2 = 0, Y = 0.019), 
                                simplify = FALSE))
pos <- 2
dfs[[pos]] <- do.call(rbind, 
                      replicate(dataWAVE$n[pos], 
                                data.frame(T1 = 0, T2 = 1, Y = 0.057), 
                                simplify = FALSE))
pos <- 3
dfs[[pos]] <- do.call(rbind, 
                      replicate(dataWAVE$n[pos], 
                                data.frame(T1 = 1, T2 = 0, Y = 0.039), 
                                simplify = FALSE))
pos <- 4
dfs[[pos]] <- do.call(rbind, 
                      replicate(dataWAVE$n[pos], 
                                data.frame(T1 = 1, T2 = 1, Y = 0.094), 
                                simplify = FALSE))
dataWAVErep <- do.call(rbind, dfs)
dataWAVErep$id <- seq_len(nrow(dataWAVErep))
```

then we use `gee`

```{r}
wave.gee <- gee::gee(formula = Y ~ T1 + T2 + T1 * T2,
                   id = id, family = gaussian, data = dataWAVErep)
summary(wave.gee)
```

Just by looking at the `Robust z` indicating that $T1:T2$ is 1.85e14 from
zero it is obvious that the interaction is statistically significant.


```{r}
coef(summary(wave.gee))
```
The actual p-value calculation (see p. 67 on how to do this) is

```{r}
z <- coef(summary(wave.gee))["T1:T2", "Robust z"]
2 * (1 - pnorm(z))
```

and since the p-value is 0 then it is statistically significant.


### Exercise 5

```{r}
dataGSS <- gss[, c("attend", "gthsedu", "female")]
dataGSS <- dataGSS[complete.cases(dataGSS), ]
dataGSS$id <- seq_len(nrow(dataGSS))
```

To determine a synergetic effect we are looking for the pattern where
$Y(t_1 = 0, t_2 = 0)  = 0$, $Y(t_1 = 1, t_2 = 0)  = 0$, $Y(t_1 = 0, t_2 = 1)  = 0$
and $Y(t_1 = 1, t_2 = 1)  = 1$. That is, as explained in p. 76 of section 4.3,
both $T_1$ and $T_2$ for $Y=1$, otherwise $Y=0$, It is the causal type 2 shown
in table 4.4 on p. 75.

Assuming monotocity we can use the result from VanderWeele and Robins (2007)
discussed in p.76 of section 4.3 stating that a sufficient condition for the 
synergistic effect is

$$
E(Y(1, 1)) - E(Y(0, 1)) - E(Y(1, 0)) + E(Y(0, 0)) > 0
$$
and if we estimate with the regression model

$$
Y = \beta_0 + \beta_1 T_1 + \beta_2 T_2 +  + \beta_3 T_1T_2
$$
then we hcan determine our result with the estimates

$$
\begin{align*}
\hat{E}(Y(0, 0)) &= \beta_0 \\
\hat{E}(Y(1, 0)) &= \beta_1 \\ 
\hat{E}(Y(0, 1)) &= \beta_2 \\
\hat{E}(Y(1, 1)) &=\beta_3
\end{align*}
$$

doing he fit with `gee`, with the binomial family because our outcome is binary, 
we obtain

```{r}
gss.gee <- gee::gee(formula = attend ~ gthsedu + female + gthsedu * female,
                   id = id, family = binomial, data = dataGSS)
summary(gss.gee)
```

so now lets compute the sufficient condition from VanderWeele and Robins (2007)
using the estimates from the regression



$$
\begin{align*}
\hat{E}(Y(1, 1)) - \hat{E}(Y(0, 1)) - \hat{E}(Y(1, 0)) + \hat{E}(Y(0, 0))
\end{align*}
$$

```{r}
Y11 <- coef(gss.gee)["gthsedu:female"]
Y01 <- coef(gss.gee)["female"]
Y10 <- coef(gss.gee)["gthsedu"]
Y00 <- coef(gss.gee)["(Intercept)"]
Y11 - Y01 - Y10 - Y00
```


Therefore the sufficient condition is not met.  We cannot positively
confirm a causal effect without more information.


### Exercise 6

Since we condition on H then we would filter the data based on H and apply
the sufficient condition for synergy to each data set filtered for each value
of H.

