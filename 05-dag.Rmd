```{r echo=TRUE, message=FALSE, warning=FALSE}
library(tidyr)
library(dplyr)
library(dagitty)
library(ggplot2)
library(ggdag)
library(gt)
library(simstudy)
```


```{r}
# directory of data files
dir_data <- file.path(getwd(), "data")
# directory for functions
dir_lib <- file.path(getwd(), "lib")
```


```{r message=FALSE, warning=FALSE, include=FALSE}
source(file = file.path(dir_lib, "ggp_dag.R"),
       local = knitr::knit_global())
source(file = file.path(dir_lib, "gt_probs.R"),
       local = knitr::knit_global())
```




# (PART) Part II {-}

# Causal Directed Acyclic Graphs {#dag}

We use the `dagitty` and `ggdag` packages to analyse the DAG and plot them.
We also sue the `simstudy` package from now on to perform the simulations.



## Theory

### Conditional Indepence and DAG

> Causal directed acyclic graphs provide a convenient and efficient way to 
represent statistical ans causal dependence.

For example

```{r warning=FALSE}
scm_5.1 <- list()
scm_5.1 <- within(scm_5.1, {
  the_nodes <- c("X1" = "", 
                 "X2" = "", 
                 "X3" = "", 
                 "X4" = "")
  dag <- dagify(
    X2 ~ X1,
    X3 ~ X2,
    X4 ~ X1 + X2,
    labels = the_nodes)
  
  text_labels <- c(expression(X[1]), expression(X[2]),
                   expression(X[3]), expression(X[4]))
  plot <- ggp_dag(dag, text_labels = text_labels)
})
```

```{r echo=TRUE, fig.align='center', fig.cap="Example of a Directed Acyclic Graph", warning=FALSE, out.width="60%"}
scm_5.1$plot
```
can be expressed in terms of conditional probabilities as

$$
\begin{align*}
P(X_1, X_2, X_3, X_4) = P(X_4 \mid X_2, X_1)P(X_3 \mid X_2)P(X_2 \mid X_1)P(X_1)
\end{align*}
$$

### D-Separation


> A path is said to be d-separated, or blocked,, by a set of variables $C$ if
and only if the path i) containis a *chain* as in figure 5.2a such that the 
midlle variable $Z$ is in $C$  or ii) contains a *fork* as in figure 5.2b such 
that such that the middle variable $Z$ is not in $C$ or iii) contains an inverted
fork, or *collider*, as in figure 5.2c such that the midlle variable $Z$ is not
in $C$ and such that no descendant of a collider is in $C$.


and we illustrate the 3 basic graphical structures as follows


```{r echo=TRUE, warning=FALSE}
scm_5.2 <- list()
scm_5.2 <- within(scm_5.2, {
  
  # CHAIN
  dagChain <- dagify(
    Z ~ X1,
    X2 ~ Z)
  
  text_labels <- c(expression(X[1]), expression(X[2]), "Z")
  plotChain <- ggp_dag(dagChain, text_size = 5, text_labels = text_labels) +
    theme(title = element_text(size = 10)) +
    labs(title = "Chain: Z is an intermediate variable")


  # FORK
  dagFork <- dagify(
    X1 ~ Z,
    X2 ~ Z)
  
  plotFork <- ggp_dag(dagFork, text_size = 5, text_labels = text_labels) +
    theme(title = element_text(size = 10)) +
    labs(title = "Fork: Z is a common cause")

  
  # COLLIDER
  dagColl <- dagify(
    Z ~ X1,
    Z ~ X2)

  plotColl <- ggp_dag(dagColl, text_size = 5, text_labels = text_labels) +
    theme(title = element_text(size = 10)) +
    labs(title = "Collider: Z is a common effect")

  
  # DESCENDANT
  dagDesc <- dagify(
    W ~ X1,
    W ~ X2,
    Z ~ W)

  text_labels <- c(expression(X[1]), expression(X[2]), expression(X[3]), "Z")
  plotDesc <- ggp_dag(dagDesc, text_size = 5, text_labels = text_labels) +
    theme(title = element_text(size = 10)) +
    labs(title = "Descendant: Z is an effect of a common effect")
})
```




```{r fig.align='center', fig.cap="Graphical structures", warning=FALSE}
gridExtra::grid.arrange(scm_5.2$plotChain, scm_5.2$plotFork, scm_5.2$plotColl, 
                        scm_5.2$plotDesc, 
                        nrow = 2, ncol = 2)
```


### Causal DAG with collider


and for our first practice of d-separation we have a DAG with a collider


```{r echo=TRUE, warning=FALSE}
scm_5.3 <- list()
scm_5.3 <- within(scm_5.3, {
  dag <- dagify(
    A ~ W1,
    Y ~ W3,
    W2 ~ W1 + W3,
    W4 ~ W2)

  text_labels <- c("A", expression(W[1]), expression(W[2]),
                   expression(W[3]), expression(W[4], "Y"))
  plot <- ggp_dag(dag, text_labels = text_labels)
})
```


```{r echo=TRUE, fig.align='center', fig.cap="Causal DAG With Collider", warning=FALSE, out.width="60%"}
scm_5.3$plot
```



The conditional independences can be obtained using `impliedConditionalIndependencies`
from the`dagitty` package. You must ensure that you use the parameter 
`type = "all.pairs` explicitly. The default is `type = "missing.edge`.
We will not list all of them, just their number, 93.

```{r}
# use type = "all.pairs" to get everything.
# impliedConditionalIndependencies uses type "missing.edge" by default.
length(impliedConditionalIndependencies(scm_5.3$dag, type = "all.pairs"))
```

some of those independence are

```{r}
impliedConditionalIndependencies(scm_5.3$dag, type = "missing.edge")
```


## Examples

### Causal DAG With Intermediate Value

Here  we have $A \not\!\perp\!\!\!\perp Y \mid M$ because $M$ is a collider and
conditioning on $M$ would open up the path.

```{r echo=TRUE, warning=FALSE}
scm_5.4 <- list()
scm_5.4 <- within(scm_5.4, {
  dag <- dagify(
    M ~ A + H,
    Y ~ A + M + H)

  plot <- ggp_dag(dag)
})
```

```{r echo=TRUE, fig.align='center', fig.cap="Causal DAG With Intermediate Variable", warning=FALSE, out.width="60%"}
scm_5.4$plot
```

### A counfounder may occur after the exposure

> Figure 5.5 illustrates and unmeasured true confounder $U$ of the effect of
$A$ on $Y$ that can be handled by conditioning on the measured variable $Z$,
even if $Z$ *occur after* $A$.

```{r echo=TRUE, warning=FALSE}
scm_5.5 <- list()
scm_5.5 <- within(scm_5.5, {

  dag <- dagify(
    A ~ U,
    Z ~ U,
    Y ~ A + Z,
    latent = "U",
    exposure = "A",
    outcome = "Y")
  
  plot <- ggp_dag(dag)
})
```


```{r}
scm_5.5$plot
```


### Potential Outcomes are the Ultimate Counfounder

> The collection of potential outcomes $\{Y(a)\}_{a \in A}$ can be viewed as the
ultimate confounder, even if it is not a tue confounder. The unmeasured 
counfounder $U = \{Y(a)\}_{a \in A}$ will always block all backdoor paths
from $A$ to $Y$, because $Y$ is a deterministic function of $A$ and $\{Y(a)\}_{a \in A}$;
that is $Y = Y(A)$, or, for binary $Y$, $Y = AY(1) + (1-A)Y(0)$

```{r echo=TRUE, warning=FALSE}
scm_5.6 <- list()
scm_5.6 <- within(scm_5.6, {
  the_nodes <- c("A" = "", 
                 "Y" = "", 
                 "U" = "(Y(0), y(1))")
  dag <- dagify(
    A ~ U,
    Y ~ A + U,
    latent = "U",
    labels = the_nodes)
  
  plot <- ggp_dag(dag)
})
```

```{r echo=TRUE, fig.align='center', fig.cap="One way to generate confounding", warning=FALSE, out.width="60%"}
scm_5.6$plot
```


and lets do the simulation of figure 5.6 on p. 88 with `simstudy`

```{r warning=FALSE}
scm_5.6 <- within(scm_5.6, {
  probY0 <- 0.42
  probY1 <- 0.62
  formulaA <- "(1-Y0) * (1- Y1) * 0.6307 + (1 - Y0) * Y1 * 0.4867 +
  Y0 * (1 - Y1) * 0.4699 + Y0 * Y1 * 0.4263"
  set.seed(111)
  # generate the potential outcomes
  defs <- defData(varname = "Y0", dist = "binomial", 
                  formula = "..probY0", variance = 1)
  defs <- defData(defs, varname = "Y1", dist = "binomial", 
                  formula = "..probY1", variance = 1)
  defs <- defData(defs, varname = "probA", dist = "nonrandom", 
                  formula = formulaA)
  defs <- defData(defs, varname = "A", dist = "binomial", 
                  formula = "probA", variance = 1)
  # Y must depend on A, Y1, Y0 in this way
  defs <- defData(defs, varname = "Y", dist = "nonrandom", 
                  formula = "A * Y1 + (1 - A) * Y0")
  
  # generate the data
  data <- genData(1000, defs)
  
  # create the tabletbl <- sim_5.6$data %>%
  tbl <- data %>%
    group_by(A, Y0, Y1, Y) %>%
    count(name = "prob") %>%
    ungroup() %>%
    mutate(prob = prob / sum(prob))
  stopifnot(near(sum(tbl$prob), 1))

  # format the tableau
  summ <- gt_probs(tbl, title = "Table 5.1",
                   subtitle = "Simulation Probabilities for `sim1.r`")
  
})
scm_5.6$summ
```

we also do it in base `R` with the script on p.88


```{r}
sim1.r <- function(n = 1000) {
  set.seed(111)
  probY0 <- 0.42
  probY1 <- 0.62
  Y0 <- rbinom(n, size = 1, prob = probY0)
  Y1 <- rbinom(n, size = 1, prob = probY1)
  probA <- (1 - Y0) * (1 - Y1) * 0.6307 + 
    (1 - Y0) * Y1 * 0.4867 +
    Y0 * (1- Y1) * 0.4699 +
    Y0 * Y1 * 0.4263
  A <- rbinom(n, size = 1, prob = probA)
  Y <- A * Y1 + (1 - A) * Y0
  data.frame(cbind(A, Y0, Y1, Y))
}
```

and we obtain the same results as with `simstudy`

```{r}
tbl <- sim1.r() %>%
  group_by(A, Y0, Y1, Y) %>%
  count(name = "prob") %>%
  ungroup() %>%
  mutate(prob = prob / sum(prob))
stopifnot(near(sum(tbl$prob), 1))
tbl
```

but when compare with the author's

```{r}
bb_5.6 <- data.frame(
  A = c(0, 0, 0, 0, 1, 1, 1, 1),
  Y0 = c(0, 0, 1, 1, 0, 0, 1, 1),
  Y1 = c(0, 1, 0, 1, 0, 1, 0, 1),
  Y = c(0, 0, 1, 1, 0, 1, 0, 1),
  prob = c(0.0814, 0.1846, 0.0846, 0.1494, 0.139, 0.175, 0.075, 0.111)
)
bb_5.6
```

it is different. Table 5.1 is entitled *Simulation probabilities for `sim1.r`*
because they are simulated probabilities.  If the simulation size is increased
10000 the results are more in line with the author's.


```{r}
data.frame("author" = bb_5.6$prob, "sim" = scm_5.6$tbl$prob) %>%
  mutate(diff = sim - author)
```

and we also confirm the theoretical probabilities of the first line in the table 
as follows


$$
\begin{align*}
\text{using (5.1), p.81, we can write the joint distribution as} \\
P(Y = 0, A = 0, Y1 = 0, Y0 = 0) = \\
P(Y = 0 \mid A = 0, Y1 = 0, Y0 = 0) P(A = 0 \mid Y1 = 0, Y0 = 0)P(Y1 = 0 \mid Y0 = 0)\ P(Y0 =0)
\end{align*}
$$

and


$$
P(Y = 0 \mid A = 0, Y1 = 0, Y0 = 0)=1
$$

and from the simulation we have


$$
\begin{align*}
P(A = 1 \mid Y0 = 0, Y1 = 0) &= 0.6307 \\
P(A = 0 \mid Y0 = 0, Y1 = 0) &=  - 0.6307 =  0.3693\\
P(A = 1 \mid Y0 = 1, Y1 = 0) &= 0.4699 \\
P(A = 0 \mid Y0 = 1, Y1 = 0) &= 1 - 0.4699 = 0.5301 \\
P(A = 1 \mid Y0 = 0, Y1 = 1) &= 0.4867 \\
P(A = 0 \mid Y0 = 0, Y1 = 1) &= 1 - 0.4867 = 0.5133 \\
P(A = 1 \mid Y0 = 1, Y1 = 1) &= 0.4263 \\
P(A = 0 \mid Y0 = 1, Y1 = 1) &= 1 - 0.4263 = 0.5737
\end{align*}
$$

and we note that $Y0$ and $Y1$ are independent, i.e. $P(Y1 \mid Y0) = P(Y1)$
so that


$$
\begin{align*}
P(Y0 = 1) &= 0.42\\
P(Y0 = 0) &= 1 - 0.42 = 0.58\\
P(Y1 = 1) &= 0.62\\
P(Y1 = 0) &= 1 - 0.62 = 0.38
\end{align*}
$$


and since $Y0$ and $Y1$ are independent, i.e. $P(Y1 \mid Y0) = P(Y1)$ then


$$
\begin{align*}
P(Y = 0, A = 0, Y1 = 0, Y0 = 0) &= \\
P(Y = 0 \mid A = 0, Y1 = 0, Y0 = 0) P(A = 0 \mid Y1 = 0, Y0 = 0) \cdot P(Y1 = 0 \mid Y0 = 0)P(Y0 = 0) &= \\
1 \cdot P(A = 0 \mid Y1 = 0, Y0 = 0) \cdot P(Y1 = 0)P(Y0 = 0) &= \\
1\times 0.3693 \times 0.58 \times 0.38 &= 0.0814 
\end{align*}
$$



### Structural Causal Model 5.7


The causal DAG is

```{r echo=TRUE}
scm_5.7 <- list()
scm_5.7 <- within(scm_5.7, {
  dag <- dagify(
    A ~ H,
    Y ~ A + H)

  plot <- ggp_dag(dag)
})
```


```{r echo=TRUE, fig.align='center', fig.cap="Another way to generate confounding", warning=FALSE, out.width="60%"}
scm_5.7$plot
```


and we simulate the model with `simstudy` (and with the author's script after)


```{r}
scm_5.7 <- within(scm_5.7, {
  set.seed(222)
  # generate the confounder H first
  defs <- defData(varname = "H", dist = "binomial", 
                  formula = 0.4, variance = 1)
  # let the treatment depend on the confounder
  formulaA <- "H * 0.8 + (1 - H) * 0.3"
  defs <- defData(defs, varname = "A", dist = "binomial", 
                  formula = formulaA, variance = 1)
  # let the outcome depend on the treatment and the confounder
  formulaY <- "A * (H * 0.5 + (1 - H) * 0.7) +
  (1 - A) * (H * 0.3 + (1 - H) * 0.5)"
  defs <- defData(defs, varname = "Y", dist = "binomial", 
                  formula = formulaY, variance = 1)
  
  # generate the data
  data <- genData(1000, defs)
  
  # create the tabletbl <- sim_5.6$data %>%
  tbl <- data %>%
    group_by(A, H, Y) %>%
    count(name = "prob") %>%
    ungroup() %>%
    mutate(prob = prob / sum(prob))
  stopifnot(near(sum(tbl$prob), 1))

  # format the tableau
  summ <- gt_probs(tbl, title = "Table 5.2",
                   subtitle = "Simulation Probabilities for `sim2.r`")
})
scm_5.7$summ
```
and with the script `sim2.r` in base R

```{r}
sim2.r <- function(n = 1000) {
  set.seed(222)
  # generate the confounder first
  H <- rbinom(n, size = 1, prob = 0.4)
  probA <- H * 0.8 + (1 - H) * 0.3
  A <- rbinom(n, size = 1, prob = probA)
  probY <- A * (H * 0.5 + (1 - H) * 0.7) + (1 - A) * (H * 0.3 + (1 - H) * 0.5)
  Y <- rbinom(n, size = 1, prob = probY)
  data.frame(cbind(H, A, Y))
}
```

with the same probabilities as `simstudy`

```{r}
tbl <- sim2.r() %>%
  group_by(A, H, Y) %>%
  count(name = "prob") %>%
  ungroup() %>%
  mutate(prob = prob / sum(prob))
stopifnot(near(sum(tbl$prob), 1))
tbl
```

and the author provides the theoretical probability table

```{r}
bb_5.7 <- data.frame(
  A = c(0, 0, 0, 0, 1, 1, 1, 1),
  H = c(0, 0, 1, 1, 0, 0, 1, 1),
  Y = c(0, 1, 0, 1, 0, 1, 0, 1),
  prob = c(0.21, 0.21, 0.056, 0.024, 0.024, 0.126, 0.16, 0.16)
)
bb_5.7
```


and computating the theoretical prob. as was donw above

$$
\begin{align*}
P(Y, A, H) = P(Y \mid A, H)P(A \mid H)P(H) \\
\end{align*}
$$

and from the simulation we have

$$
\begin{align*}
P(H = 1) = 0.4 \\
P(H = 0) = 0.6 \\
P(A = 1 \mid H = 0) = 0.3 \\
P(A = 0 \mid H = 0) = 1 - 0.3 = 0.7 \\
P(A = 1 \mid H = 1) = 0.8 \\
P(A = 0 \mid H = 1) = 1 - 0.8 = 0.2 \\
P(Y = 1 \mid A = 1, H = 1) = 0.5\\
P(Y = 0 \mid A = 1, H = 1) = 1 - 0.5 = 0.5\\
P(Y = 1 \mid A = 1, H = 0) = 0.7\\
P(Y = 0 \mid A = 1, H = 0) = 1 - 0.7 = 0.3\\
P(Y = 1 \mid A = 0, H = 1) = 0.3\\
P(Y = 0 \mid A = 0, H = 1) = 1 - 0.3 = 0.7\\
P(Y = 1 \mid A = 0, H = 0) = 0.5\\
P(Y = 0 \mid A = 0, H = 0) = 1 - 0.5 = 0.5\\
\end{align*}
$$

Therefore for row 1 of table 5.2 we have

$$
\begin{align*}
P(Y = 0, A = 0, H = 0) &= P(Y = 0 \mid A = 0, H = 0)P(A = 0 \mid H = 0)P(H = 0) \\
&= 0.5 \times 0.7 \times 0.6 = 0.21
\end{align*}
$$


and for row 4 of table 5.2


$$
\begin{align*}
P(Y = 1, A = 0, H = 1) &= P(Y = 1 \mid A = 0, H = 1)P(A = 0 \mid H = 1)P(H = 1) \\
&= 0.3 \times 0.2 \times 0.4 = 0.024
\end{align*}
$$


Using either `sim1.r` or `sim2.r` we can get the joint probability of $A$ and 
$Y$ using the law of total probabilities.


```{r}
bb_5.6 %>%
  group_by(A, Y) %>%
  summarize(prob = sum(prob))
```

```{r}
bb_5.7 %>%
  group_by(A, Y) %>%
  summarize(prob = sum(prob))
```


Now

> One might wonder if knowledge of $(A, H, Y)$ is equivalent to knowledge
of $(A, Y0, Y1, Y)$ Given $(A, Y)$ we cannot recover both $Y0$ and $Y1$.

For example we can recover $Y1 = 1$ since 

$$
\begin{align*}
P(Y1 = 1 \mid A = 1, Y = 1) &= \frac{P(Y1 = 1, A = 1, Y = 1)}{P(A = 1, Y = 1)} \\
&= \frac{0.175 + 0.111}{0.175 + 0.111} = 1
\end{align*}
$$

but for $P(Y0 = 1 \mid A = 1, Y = 1)$ we have


$$
\begin{align*}
P(Y0 = 1 \mid A = 1, Y = 1) &= \frac{P(Y0 = 1, A = 1, Y = 1)}{P(A = 1, Y = 1)} \\
&= \frac{0.111}{0.175 + 0.111} = 0.39
\end{align*}
$$

and so the likelihood that $Y0 = 1$ is only 39% and it is therefore more likely
that $Y0 = 0$ . . . but not always.


> Does the additional knowledge of $H$ help to identify $Y(0)$? We leave that 
as a question  for the reader.

If we include $H$ in the DAG of figure 5.6 we obtain the DAG of figure
5.8 below. We observe that $(Y(0), Y(1)) \perp\!\!\!\perp A \mid H$. In addition,
since $Y$ is a collider $(Y(0), Y(1)) \not\!\perp\!\!\!\perp A \mid Y$, therefore
if we use the data from figure 5.8 below (`simall.r`) we have


$$
\begin{align*}
P(Y0 = 1 \mid H = 1) &= \frac{P(Y0 = 1, H = 1)}{P(H = 1)} \\
&= \frac{0.019 + 0.006 + 0.051 + 0.05}{0.031 + 0.02 + 0.019 + 0.006 + 0.109 + 0.117 + 0.051 + 0.05} \\
&= \frac{0.126}{0.403} = 0.31
\end{align*}
$$
Therefore $H$ is not entirely sufficient to determine $(Y(0), Y(1))$ as it
will do it only 31% of the time for $Y(0) = 1$ and otherwise 69% of the time
for $Y(0) = 0$.




### Potential Outcomes Behind the Scenes



```{r echo=TRUE, warning=FALSE}
scm_5.8 <- list()
scm_5.8 <- within(scm_5.8, {
  the_nodes <- c("A" = "", 
                 "Y" = "", 
                 "U" = "(Y(0), y(1))")
  dag <- dagify(
    A ~ H,
    Y ~ A + U,
    U ~ H,
    latent = "U",
    labels = the_nodes)
  
  plot <- ggp_dag(dag)
})
```


```{r echo=TRUE, fig.align='center', fig.cap="Potential Outcomes Behind the Scenes", warning=FALSE, out.width="60%"}
scm_5.8$plot
```



```{r}
scm_5.8 <- within(scm_5.8, {
  set.seed(888)
  # generate the observed confounder H
  defs <- defData(varname = "H", dist = "binomial", 
                  formula = 0.4, variance = 1)
  # let the treatment depend on the observed confounder
  formulaA <- "H * 0.8 + (1 - H) * 0.3"
  defs <- defData(defs, varname = "A", dist = "binomial", 
                  formula = formulaA, variance = 1)
  # generate the poential outcomes dependent on the observed confounder
  formulaY0 <- "H * 0.3 + (1 - H) * 0.5"
  formulaY1 <- "H * 0.5 + (1 - H) * 0.7"
  defs <- defData(defs, varname = "Y0", dist = "binomial", 
                  formula = formulaY0, variance = 1)
  defs <- defData(defs, varname = "Y1", dist = "binomial", 
                  formula = formulaY1, variance = 1)
  # let the outcome depend on the treatment and Y0, Y1
  formulaY <- "A * Y1 + (1 - A) * Y0"
  defs <- defData(defs, varname = "Y", dist = "binomial", 
                  formula = formulaY, variance = 1)
  
  # generate the data
  data <- genData(1000, defs)
  
  # create the tabletbl <- sim_5.6$data %>%
  tbl <- data %>%
    group_by(H, A, Y0, Y1, Y) %>%
    count(name = "prob") %>%
    ungroup() %>%
    mutate(prob = prob / sum(prob))
  stopifnot(near(sum(tbl$prob), 1))

  # format the tableau
  tbl <- gt_probs(tbl, title = "Table for `simall.r`",
                   subtitle = "Simulation Probabilities for `simall.r`")
})
scm_5.8$tbl
```

and with the script `simall.r` in base R


```{r}
simall.r <- function(n = 1000) {
  set.seed(888)
  # generate the observed confounder
  H <- rbinom(n, size = 1, prob = 0.4)
  # generate the treatment
  probA <- H * 0.8 + (1 - H) * 0.3
  A <- rbinom(n, size = 1, prob = probA)
  # generate the poential outcomes dependent on the observed confounder
  probY0 <- H * 0.3 + (1 - H) * 0.5
  probY1 <- H * 0.5 + (1 - H) * 0.7
  Y0 <- rbinom(n, size = 1, prob = probY0)
  Y1 <- rbinom(n, size = 1, prob = probY1)
  # let the outcome depend on the treatment and Y0, Y1
  probY <- A * Y1 + (1 - A) * Y0
  Y <- rbinom(n, size = 1, prob = probY)
  data.frame(cbind(H, A, Y0, Y1, Y))
}
```


and the causal DAG for the What-If? study is


```{r echo=TRUE, fig.align='center', fig.cap="The Double What-If? Study", warning=FALSE, out.width="80%"}
scm_5.9 <- list()
scm_5.9 <- within(scm_5.9, {
  the_nodes <- c("U" = "Unmeasured, healthy behavior (U=1)", 
                 "AD0" = "Adherence time 0", 
                 "VL0" = "Viral Load time 0", 
                 "T" = "Naltrexone (T=1)", 
                 "A" = "Reduced drinking (A=1)", 
                 "AD1" = "Adherence time 1", 
                 "VL1" = "Viral Load time 1")
  coords <- data.frame(
    name = names(the_nodes),
    x = c(2, 3, 4, 1, 2, 3, 4),
    y = c(2, 2, 2, 1, 1, 1, 1)
  )
  dag <- dagify(
    AD0 ~ U,
    VL0 ~ AD0,
    A ~ `T` + U,
    AD1 ~ A,
    VL1 ~ AD0 + AD1 + U,
  outcome = "VL1",
  exposure = "T",
  latent = "U",
  labels = the_nodes)
  
  
  text_labels <- c("A", expression(AD[0]), expression(AD[1]),
                   "T", "U", expression(VL[0]), expression(VL[1]))
  plot <- ggp_dag(dag, text_labels = text_labels, text_size = 5)
})
scm_5.9$plot
```



## Exercises

### Exercise 1



```{r echo=TRUE, warning=FALSE}
scm_ex5.1 <- list()
scm_ex5.1 <- within(scm_ex5.1, {
    the_nodes <- c("A" = "Nutrient",
                   "W1" = "Diet",
                   "W2" = "Weight",
                   "W3" = "Exercise",
                   "Y" = "Strength")
  dag <- dagify(
    A ~ W1,
    Y ~ W3,
    W2 ~ W1 + W3,
    labels = the_nodes,
    exposure = "A",
    outcome = "Y")

  
  text_labels <- c("A", expression(W[1]), expression(W[2]),
                   expression(W[3]), "Y")
  plot <- ggp_dag(dag, text_labels = text_labels, box_padding = 3)
})
```


> Math the vairable to the causal DAG



```{r echo=FALSE, fig.align='center', fig.cap="Causal DAG for Exercise 1", warning=FALSE, out.width="80%"}
scm_ex5.1$plot
```


> How wwould you analyse the effect of nutrient intake on increased strength?

Since we have a collider at $W_3 = weight$ then $diet$ and $exercise$ are not
independent and $weight$ is a confounder if we condition on it. That is




$$
Strength \not \! \perp\!\!\!\perp Nutrient \mid Weight
$$

So the effect of $nutrient$ on $strength$ would be measured without
conditioning on $weight$. The regression

$$
strength = \beta_0 + \beta_1 nutrient
$$
The analysis would be conducted by evaluating the effect measures as described 
in chapter 3 and 4.


> How might a reviewer critique your results

The reviewer could raise the following points

* Several regression models including different mixtures of predictor
variables should be run and evaluated toe stimate the impact of confounders
* Excluding other predictors in the regression models could make the analysis 
spurious as the effects of confounders might actually be determinant.


### Exercise 2

The DAG is 


```{r echo=TRUE, warning=FALSE}
scm_ex5.2 <- list()
scm_ex5.2 <- within(scm_ex5.2, {
  the_nodes <- c("A" = "", "H1" = "", "H2" = "", 
                 "H3" = "", "H4" = "", "Y" = "")
  dag <- dagify(
    H2 ~ H1,
    H3 ~ H1,
    H4 ~ H1 + H3,
    A ~ H2 + H3,
    Y ~ A + H2 + H4,
    exposure = "A",
    outcome = "Y",
    labels = the_nodes)

  text_labels <- c("A", expression(H[1]), expression(H[2]),
                   expression(H[3]), expression(H[4]), "Y")
  plot <- ggp_dag(dag, text_labels = text_labels)
})
```



```{r echo=FALSE, fig.align='center', fig.cap="Causal DAG for Exercise 2", warning=FALSE, out.width="80%"}
scm_ex5.2$plot
```


> Which varibales are true confounder?

A true confounder is defined on p. 86 as

> a variable that influences the exposure and that also influences the outcome
via a directed path that does not include the exposure.

$H1$ is a true confounder since
* It influences the exposure $A$ through the path $H_1 \rightarrow H_3 \rightarrow A$
and influences the outcome through the path $H_1 \rightarrow H_4 \rightarrow Y$ 
that does not include $A$ 

$H2$ is a true confounder since
* It influences the exposure $A$ through the path $H_2 \rightarrow A$
and influences the outcome through the path $H_2 \rightarrow Y$ 
that does not include $A$

$H3$ is a true confounder since
* It influences the exposure $A$ through the path $H_3 \rightarrow A$
and influences the outcome through the path $H_3 \rightarrow H_4 \rightarrow Y$ 
that does not include $A$


> Is the set $\{H_2, H_4\}$ a sufficient set of true confounders?

The minimal adjustment sets, also called smallest sufficient set of true
confounders, include the set $\{H_2, H_4\}$ so the answer is yes.

```{r}
adjustmentSets(scm_ex5.2$dag, exposure = "A", outcome = "Y")
```


> Redraw the DAG to include potential outcomes behind the scene.

We block $\{H_2, H_4\}$ with the potential outcomes.


```{r echo=FALSE, fig.align='center', fig.cap="Causal DAG for Exercise 2 with Potential Outcomes", warning=FALSE, out.width="80%"}
scm_ex5.2po <- list()
scm_ex5.2po <- within(scm_ex5.2po, {
    the_nodes <- c("A" = "",
                   "H1" = "",
                   "H2" = "",
                   "H3" = "",
                   "H4" = "",
                   "U" = "(Y(0), y(1))",
                   "Y" = "")
  dag <- dagify(
    H2 ~ H1,
    H3 ~ H1,
    H4 ~ H1 + H3,
    A ~ H2 + H3,
    U ~ H2 + H4,
    Y ~ A + U,
    exposure = "A",
    latent = "U",
    outcome = "Y",
    labels = the_nodes)

  text_labels <- c("A", expression(H[1]), expression(H[2]),
                   expression(H[3]), expression(H[4]), "U", "Y")
  plot <- ggp_dag(dag, text_labels = text_labels, text_size = 5)
})
scm_ex5.2po$plot
```




> Are the potenial outcomes true confounders?

No as they do not influence the exposure. See also section 5.2 p. 87, after 
figure 5.5, that says

> The collection of potential outcomes $\{Y(a)\}_{a \in A}$ van be viewed as the 
ultimate confounder, even *when it is not a true confounder*.

And since the potential outcome is a latent variable it is not included in
the minimal set. But the minimal set is clearly block by the potential outcomes node $U$.

```{r}
adjustmentSets(scm_ex5.2po$dag)
```



### Exercise 3

The simulation is done with `simstudy`


```{r warning=FALSE}
scm_ex5.3 <- list()
scm_ex5.3 <- within(scm_ex5.3, {
  set.seed(1234)
  defs <- defData(varname = "H1", dist = "binomial", 
                  formula = 0.1, variance = 1)
  formulaH2 <- "0.1 + 0.1 * H1"
  defs <- defData(defs, varname = "H2", dist = "binomial", 
                  formula = formulaH2, variance = 1)
  formulaH3<- "0.2 + 0.2 * H2"
  defs <- defData(defs, varname = "H3", dist = "binomial", 
                  formula = formulaH3, variance = 1)
  defs <- defData(defs, varname = "H4", dist = "binomial", 
                  formula = 0.5, variance = 1)
  formulaA <- "0.1 + 0.1 * H2 + 0.1 * H3 + 0.1 * H4"
  defs <- defData(defs, varname = "A", dist = "binomial", 
                  formula = formulaA, variance = 1)
  formulaY <- "0.1 + 0.2 * H1 + 0.2 * H2 + + 0.3 * A"
  defs <- defData(defs, varname = "Y", dist = "binomial", 
                  formula = formulaY, variance = 1)
  
  # generate the data
  data <- genData(1000, defs)
  
  # create table of joint probabilities
  tbl <- data %>%
    group_by(H1, H2, H3, H4, A, Y) %>%
    count(name = "prob") %>%
    ungroup() %>%
    mutate(prob = prob / sum(prob))
  stopifnot(near(sum(tbl$prob), 1))

  # format the tableau
  summ <- gt_probs(tbl, title = "Table exercise 5.3",
                   subtitle = "Simulation Probabilities for `simex3.r`")
})
```

> Draw a causal DAG

```{r}
scm_ex5.3 <- within(scm_ex5.3, {
  the_nodes <- c("A" = "", "H1" = "", "H2" = "", 
                 "H3" = "", "H4" = "", "Y" = "")
  dag <- dagify(
    H2 ~ H1,
    H3 ~ H2,
    A ~ H2 + H3 + H4,
    Y ~ A + H1 + H2,
    exposure = "A",
    outcome = "Y",
    labels = the_nodes)
  
  text_labels <- c("A", expression(H[1]), expression(H[2]),
                            expression(H[3]), expression(H[4]), "Y")
  plot <- ggp_dag(dag, layout = "nicely", text_labels = text_labels, text_size = 5)
})
```


```{r echo=FALSE, fig.align='center', fig.cap="Causal DAG for Exercise 3", warning=FALSE, out.width="80%"}
scm_ex5.3$plot
```

> List the true confounders

it is worth repeating the definition of true confounders found on p. 86

> a variable that influences the exposure and that also influences the outcome
via a directed path that does not include the exposure.

which in this case are $\{H_1, H_2\}$ because

* $H_1$ influence the exposure through $H_1 \rightarrow H_2 \rightarrow A$ and
the outcome without the exposure in the path through $H_1 \rightarrow Y$
* $H_2$ influence the exposure through $H_2 \rightarrow A$ and
the outcome without the exposure in the path through $H_2 \rightarrow Y$

> List the smallest sufficient set of true confounders

That set would be $\{H_2\}$ since it blocks $H_1$ as well

```{r}
adjustmentSets(scm_ex5.3$dag)
```

Therefore to redraw with potential outcomes we would blovk the paths $H_2 \rightarrow Y$
and $H_1 \rightarrow Y$ as follows


```{r}
scm_ex5.3po <- list()
scm_ex5.3po <- within(scm_ex5.3po, {
  the_nodes <- c("A" = "", "H1" = "", "H2" = "", 
                 "H3" = "", "H4" = "", "U" = "(Y(0), Y(1))", "Y" = "")
  dag <- dagify(
    H2 ~ H1,
    H3 ~ H2,
    A ~ H2 + H3 + H4,
    U ~ H1 + H2,
    Y ~ A + U,
    exposure = "A",
    outcome = "Y",
    latent = "U",
    labels = the_nodes)
  
  text_labels <- c("A", expression(H[1]), expression(H[2]),
                            expression(H[3]), expression(H[4]), "U", "Y")
  plot <- ggp_dag(dag, layout = "nicely", text_labels = text_labels, text_size = 5)
  })
```


```{r echo=FALSE, fig.align='center', fig.cap="Causal DAG for Exercise 3 With Potential Outcomes", warning=FALSE, out.width="80%"}
scm_ex5.3po$plot
```

> Verify that they form a sufficient set of confounders

and again the minimal set is $\{H_2\}$ but this time it is blocked by the latent
variable $U$.


```{r}
adjustmentSets(scm_ex5.3po$dag)
```

> Are the potential outcomes true confounders?

No as they do not influence the exposure. See also section 5.2 p. 87, after 
figure 5.5, that says

> The collection of potential outcomes $\{Y(a)\}_{a \in A}$ van be viewed as the 
ultimate confounder, even *when it is not a true confounder*.


### Exercise 4



```{r}
scm_ex5.4 <- list()
scm_ex5.4 <- within(scm_ex5.4, {
  the_nodes <- c("O" = "", "T" = "", "Y" = "")
  dag <- dagify(
    Y ~ O + `T`,
    exposure = "T",
    outcome = "Y",
    labels  = the_nodes)
  
  plot <- ggp_dag(dag, text_size = 5)
})
```


```{r echo=FALSE, fig.align='center', fig.cap="Causal DAG for Exercise 4", warning=FALSE, out.width="80%"}
scm_ex5.4$plot
```

> Explain why assessing safety of the vaccine by estimating the association between 
$T$ and $Y$ for those with $O = 1$ is a biased analysis.

By including $O = 1$ in the analysis $Y$ becomes a *common effect* of both
$O$ and $T$. Therefore conditioning on $O$ cause the study to focus on a slice
of data that itself might influence the outcome since the fact that some
participants are observed but not others might very well not be random and
the result of one or many latent confounders.


### Exercise 5


```{r}
scm_ex5.5 <- list()
scm_ex5.5 <- within(scm_ex5.5, {
  
  # the simulation
  set.seed(2468)
  defs <- defData(varname = "T", dist = "binomial", 
                  formula = 0.5, variance = 1)
  formulaY <- "0.01 * T"
  defs <- defData(defs, varname = "Y", dist = "binomial", 
                  formula = formulaY, variance = 1)
  formulaO <- "Y + 0.1 * (1- Y)"
  defs <- defData(defs, varname = "O", dist = "binomial", 
                  formula = formulaO, variance = 1)
  
  # generate the data
  data <- genData(1e4, defs)
  
  # create table of joint probabilities
  tbl <- data %>%
    group_by(`T`, Y, O) %>%
    count(name = "prob") %>%
    ungroup() %>%
    mutate(prob = prob / sum(prob))
  stopifnot(near(sum(tbl$prob), 1))

  # format the tableau
  summ <- tbl_probs(tbl, title = "Table exercise 5.5",
                   subtitle = "Simulation Probabilities for `simex5.r`")
  
  
  # The DAG
  the_nodes <- c("O" = "Inclusion", "T" = "Infection HPV", "Y" = "Cervical Cancer")
  dag <- dagify(
    Y ~ `T`,
    O ~ Y, 
    exposure = "T",
    outcome = "Y",
    labels  = the_nodes)
  
  plot <- ggp_dag(dag, text_size = 5)
})
```


```{r echo=FALSE, fig.align='center', fig.cap="Causal DAG for Exercise 5", warning=FALSE, out.width="80%"}
scm_ex5.5$plot
```

As explained in section 3.3 of chapter 3 if we use the information from the
simulation then

$$
\begin{align*}
p_0 &= E(Y = 1 \mid T = 0) = 0.01 * 0.5 = 0.005 \\
p_1 &= E(Y = 1 \mid T = 1) = 0.01 \\
&\therefore \\
RD &= 0.01 - 0.005 = 0.005 \\
OR &= \frac{\frac{p_1}{1 - p_1}}{\frac{p_0}{1-p_0}} = \frac{0.1}{0.9} \cdot \frac{0.995}{0.005} = 22\frac{1}{9} =22.11
\end{align*}
$$

#### a)

Show mathematically that the odds ratio comparing with $O$ is the same as when
not conditioning  with $O$

The odds ratio $OR$ is

$$
\begin{align*}
OR &= \frac{P(Y=1 \mid T = 1, O =1)}{1 - P(Y=1 \mid T = 1, O =1)} \cdot \frac{1 - P(Y=1 \mid T=0, O =1)}{P(Y=1 \mid T=0, O =1)} \\
& =\frac{P(Y=1 \mid T = 1, O =1)}{P(Y=0 \mid T = 1, O =1)} \cdot \frac{P(Y=0 \mid T = 0, O =1)}{P(Y = 1 \mid T = 0, O =1)} \\
\end{align*}
$$
but we know that

$$
\begin{align*}
P(Y \mid T, O)P(T, O) = P(T, Y, O) = P(T \mid Y, O) P(Y, O)
\end{align*}
$$
therefore


$$
\begin{align*}
OR &= \frac{P(Y=1 \mid T=1, O=1)}{P(Y=0 \mid T=1, O=1)} \cdot \frac{P(Y=0 \mid T=0, O=1)}{P(Y=1 \mid T=0, O=1)} \\
&= \frac{P(T=1 \mid Y=1, O=1)}{P(T=1 \mid Y=0, O=1)} \cdot \frac{P(T=0 \mid Y=0, O=1)}{P(T=0 \mid Y=1, O =1)}
\end{align*}
$$
ans since

$$
T\perp\!\!\!\perp O \mid Y \implies P(T \mid Y, O) = P(T \mid Y)
$$

then



$$
\begin{align*}
OR &= \frac{P(T=1 \mid Y=1, O=1)}{P(T=1 \mid Y=0, O=1)} \cdot \frac{P(T=0 \mid Y=0, O=1)}{P(T=0 \mid Y=1, O=1)} \\
&= \frac{P(T=1 \mid Y=1)}{P(T=1 \mid Y=0)} \cdot \frac{P(T=0 \mid Y=0)}{P(T=0 \mid Y=1)}
\end{align*}
$$

and again by using

$$
\begin{align*}
P(Y \mid T)P(T) = P(T, Y) = P(T \mid Y) P(Y)
\end{align*}
$$
we have

$$
\begin{align*}
OR &= \frac{P(T=1 \mid Y=1)}{P(T=1 \mid Y=0)} \cdot \frac{P(T=0 \mid Y=0)}{P(T=0 \mid Y=1)} \\
&= \frac{P(Y=1 \mid T=1)}{P(Y=1 \mid T=0)} \cdot \frac{P(Y=0 \mid T=0)}{P(Y=0 \mid T=1)}
\end{align*}
$$

#### b)

Show that when $Y=1$ is rare, the odds ratio is similar to the risk ratio.

from a) above we have

$$
\begin{align*}
OR &= \frac{P(Y=1 \mid T=1)}{P(Y=1 \mid T=0)} \cdot \frac{P(Y=0 \mid T=0)}{P(Y=0 \mid T=1)}
\end{align*}
$$
and since $Y$ is rare


$$
P(Y=0 \mid T=0)\approx 1 \approx P(Y=0 \mid T=1)
$$

therefore


$$
\begin{align*}
OR &= \frac{P(Y=1 \mid T=1)}{P(Y=1 \mid T=0)} \cdot \frac{P(Y=0 \mid T=0)}{P(Y=0 \mid T=1)} \\
&\approx \frac{P(Y=1 \mid T=1)}{P(Y=1 \mid T=0)} \\
&= RR
\end{align*}
$$

